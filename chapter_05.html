<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Chapter 5: FoSR</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" />
<script defer src="https://use.fontawesome.com/releases/v5.0.3/js/all.js"></script>
<script defer src="https://use.fontawesome.com/releases/v5.0.0/js/v4-shims.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151578452-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-151578452-1');
</script>
-->

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">FDA with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="overview.html">Overview</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Datasets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="dataset_nhanes.html">NHANES</a>
    </li>
    <li>
      <a href="dataset_content.html">Content</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Chapters
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="chapter_01.html">Chapter 1</a>
    </li>
    <li>
      <a href="chapter_02.html">Chapter 2</a>
    </li>
    <li>
      <a href="chapter_03.html">Chapter 3: FPCA</a>
    </li>
    <li>
      <a href="chapter_04.html">Chapter 4: SoFR</a>
    </li>
    <li>
      <a href="chapter_05.html">Chapter 4: FoSR</a>
    </li>
  </ul>
</li>
<li>
  <a href="scripts.html">Scripts</a>
</li>
<li>
  <a href="https://github.com/FDAwithR">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Chapter 5: FoSR</h1>

</div>


<p>We now consider the use of functions as responses in models with
scalar predictors. This setting is widespread in applications of
functional data analysis, and builds on specific tools and broader
intuition developed in previous chapters. We will focus on the linear
Function-on-Scalar Regression (FoSR) model, with a brief overview of
alternative approaches in later sections.</p>
<div id="motivation-and-exploratory-analysis-of-mims-profiles"
class="section level2">
<h2>Motivation and Exploratory Analysis of MIMS Profiles</h2>
<p>The code below imports and organizes data that will be used in this
Chapter. As elsewhere, we begin with the processed NHANES data. The
<code>MIMS</code> variable is converted to a <code>tidyfun</code> object
and stored as a single column of functional observations; we retain a
small number of scalar covariates (<code>gender</code> and
<code>age</code>, as well as the participant ID in <code>SEQN</code>
which we convert to a factor variable) and create a categorical age
variable. Subjects who are missing covariate information are dropped. To
keep the examples in this section computationally feasible, we use the
first 250 subjects in this dataset.</p>
<pre class="r"><code># import and organize the data

nhanes_df = 
  readRDS(
    here::here(&quot;data&quot;, &quot;nhanes_fda_with_r.rds&quot;)) %&gt;% 
  select(SEQN, gender, age, MIMS_mat = MIMS) %&gt;% 
  mutate(
    age_cat = 
      cut(age, breaks = c(18, 35, 50, 65, 80), 
          include.lowest = TRUE)) %&gt;% 
  drop_na(age, age_cat) %&gt;% 
  filter(age &gt;= 25) %&gt;% 
  tibble() %&gt;% 
  slice(1:250) </code></pre>
<pre class="r"><code>nhanes_df = 
  nhanes_df %&gt;% 
  mutate(
    MIMS_tf = matrix(MIMS_mat, ncol = 1440),
    MIMS_tf = tfd(MIMS_tf, arg = seq(1/60, 24, length = 1440)))</code></pre>
<p>Physical activity trajectories observed over 24 hours are shaped by
subject-level characteristics, and better understanding of the drivers
of activity may provide useful insights into health behaviors or
potential interventions that promote physical activity. One way to
explore these complex associations is to consider the MIMS profile as a
response that varies according to scalar predictors like age and gender.
The Figure below is similar to one shown in the Introduction, and shows
minute-level averages for participants in each age category separately
for male and female. From this, we observe a clear diurnal pattern of
activity, and note a decrease in physical activity in the afternoon and
evening as age increases this is particularly noticeable for male
participants. We also see that female participants generally have a
higher mean activity than male participants in the same age
category.</p>
<pre class="r"><code># exploratory data analysis and plot
nhanes_df %&gt;% 
  group_by(age_cat, gender) %&gt;% 
  summarize(mean_mims = mean(MIMS_tf)) %&gt;% 
  ggplot(aes(y = mean_mims, color = age_cat)) + 
  geom_spaghetti() + 
  facet_grid(.~gender) + 
  scale_x_continuous(breaks = seq(0, 24, length = 5)) + 
  labs(x = &quot;Time of day (hours)&quot;, y = &quot;Average MIMS&quot;)
## `summarise()` has grouped output by &#39;age_cat&#39;. You can override using the
## `.groups` argument.</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-2-1.png" width="90%" /></p>
<pre><code>## `summarise()` has grouped output by &#39;age_cat&#39;. You can override using the
## `.groups` argument.</code></pre>
<p>Exploratory plots like this one are useful for beginning to
understand the effects of covariates on physical activity, but are
limited in their scope. We are unable, from this Figure, to adjust for
covariates or understand issues like confounding, and there is no direct
mechanism to assess statistical significance or make inferences.
Although the means in adjacent minutes are similar due to the underlying
structure of the observations, simple minute-level averages do not
explicitly leverage that structure. These are some of the issues that
more formal approaches to function-on-scalar regression are intended to
address.</p>
<div id="regressions-using-binned-data" class="section level3">
<h3>Regressions using Binned Data</h3>
<p>To build intuition for functional response model interpretations,
identify practical challenges, and motivate later developments, we fit a
series of regressions to MIMS profiles using binned or aggregate
observations. As a first step, we aggregate minute-level data into
hour-length bins, so that each MIMS profile consists as 24 observations.
To obtain hourly trajectories, we “smooth” minute-level data using a
rolling mean with a 60-minute bandwidth, and evaluate the resulting
functions at the midpoint of each hour.</p>
<pre class="r"><code>#Construct a variable that contains average MIMS for each hour

nhanes_df = 
  nhanes_df %&gt;% 
  mutate(
    MIMS_hour = 
      tf_smooth(MIMS_tf, method = &quot;rollmean&quot;, k = 60, align = &quot;center&quot;),
    MIMS_hour = tfd(MIMS_hour, arg = seq(.5, 23.5, by = 1))) 
## setting fill = &#39;extend&#39; for start/end values.
## Warning: There was 1 warning in `mutate()`.
## ℹ In argument: `MIMS_hour = tf_smooth(MIMS_tf, method = &quot;rollmean&quot;, k = 60,
##   align = &quot;center&quot;)`.
## Caused by warning in `tf_smooth.tfd()`:
## ! non-equidistant arg-values in &#39;MIMS_tf&#39; ignored by rollmean.</code></pre>
<p>The resulting <code>MIMS_hour</code> data are shown for a subset of
participants in the Figure below. As before, we are interested in the
effects of <code>age</code>, now treated as a continuous variable, and
<code>gender</code>. The binning process retains a level of granularity
that is informative regarding diurnal patterns of activation but
reflects a substantial reduction in the dimension and detail in the
data.</p>
<pre class="r"><code>#Plot subject-specific MIMS average within each hour

nhanes_df %&gt;% 
  ggplot(aes(y = MIMS_hour, color = age)) + 
  geom_spaghetti(alpha = .2) + 
  geom_meatballs(alpha = .2) + 
  facet_grid(.~gender)</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-5-1.png" width="90%" /></p>
<p>These data can be analyzed using hour-specific linear models that
regress bin-average MIMS values on <code>age</code> and
<code>gender</code>. This collection of linear models does not account
for the temporal structure of the diurnal profiles except through the
binning that aggregates data to an hour level, but taken together will
illustrate the association between the outcome and predictors over the
course of the day.</p>
<p>As a first example, we’ll fit a standard linear model for with the
average MIMS value between 1:00 and 2:00pm as a response. To do this, we
unnest the hour-level functional observations and subset to the interval
of current interest. These data are shown as a scatterplot below, with
the fit from a linear model overlaid.</p>
<pre class="r"><code>#Fit a linear model for average MIMS between 1.00 and 2.00pm

linear_fit = 
  nhanes_df %&gt;% 
  select(SEQN, age, gender, MIMS_hour) %&gt;% 
  tf_unnest(MIMS_hour) %&gt;% 
  filter(MIMS_hour_arg == 13.5) %&gt;% 
  lm(MIMS_hour_value ~ age + gender, data = .)</code></pre>
<p>Make a table of results.</p>
<p>Make a plot of results</p>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-9-1.png" width="90%" /></p>
<p>The scatterplot and regression results are consistent with our
previous observations: the binned-average MIMS values decrease with age,
and female participants may have somewhat higher values than male
participants. The benefit of this analysis over visualization-based
exploratory techniques is that it provides a formal statistical
assessment of these effects and their significance.</p>
<p>Associations between binned-average MIMS values and <code>age</code>
and <code>gender</code> will vary by hour, and the next step in our
exploratory analysis is to fit separate regressions at each hour
separately. We accomplish this using data nested within hour. First,
we’ll unnest the subject-specific functional observation and then
re-nest within hour; the result is a dataframe containing 24 rows, one
for each hour, with a column that contains a list of hour-specific
dataframes containing the <code>MIMS_hour_value</code>,
<code>age</code>, and <code>gender</code>. By mapping over the entries
in this list, we can fit hour-specific linear models and extract tidied
results. The code chunk below implements this analysis.</p>
<pre class="r"><code>#Fit a linear model for average MIMS at each hour of the day

hourly_regressions = 
  nhanes_df %&gt;% 
  select(SEQN, age, gender, MIMS_hour) %&gt;% 
  tf_unnest(MIMS_hour) %&gt;% 
  rename(hour = MIMS_hour_arg, MIMS = MIMS_hour_value) %&gt;% 
  nest(data = -hour) %&gt;% 
  mutate(
    model = map(.x = data, ~lm(MIMS ~ age + gender, data = .x)),
    result = map(model, broom::tidy)
  ) %&gt;% 
  select(hour, result)</code></pre>
<p>Before visualizing the results, we do some data processing to obtain
hourly confidence intervals and then structure coefficient estimates and
confidence bands as <code>tf</code> objects. The result is a three-row
dataframe, with rows for the intercept, age, and gender effects; columns
include the term name as well as the coefficient estimates and the upper
and lower limits of the confidence bands.</p>
<pre class="r"><code>#Obtain confidence intervals and organize regression results

hour_bin_coefs = 
  hourly_regressions %&gt;% 
  unnest(cols = result) %&gt;% 
  rename(coef = estimate, se = std.error) %&gt;% 
  mutate(
    ub = coef + 1.96 * se, 
    lb = coef - 1.96 * se
  ) %&gt;% 
  select(hour, term, coef, ub, lb) %&gt;% 
  tf_nest(coef:lb, .id = term, .arg = hour)</code></pre>
<p>We show the analysis results using <code>ggplot</code> and
<code>tidyfun</code> functions. Because the coefficient estimates in
<code>coef</code> are <code>tf</code> objects, we can plot these using
<code>geom_spaghetti</code>; to emphasize the estimates at each hour, we
add points using <code>geom_meatballs</code>. Confidence bands are shown
by specifying the upper and lower limits, and we facet by term to see
each coefficient separately.</p>
<pre class="r"><code># plot coefficients and CIs

hour_bin_coefs %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti() +  
  geom_meatballs() + 
  geom_errorband(aes(ymax = ub, ymin = lb, fill = term)) + 
  facet_wrap(&quot;term&quot;, scales = &quot;free&quot;)</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-12-1.png" width="90%" /></p>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-13-1.png" width="90%" /></p>
<p>Compared to the regression at a single time point, these results
provide detailed temporal information about covariate effects. The
intercept shows a fairly typical diurnal pattern, with low activity in
the night and higher activity in the day. After adjusting for age, women
are somewhat less active in the middle of the night but more active
during the daytime hours. Meanwhile, after adjusting for gender, older
participants are generally less active at all times. The associated
confidence bands suggest that not all of these effects are significant –
there may not be a significant effect of age in the early morning, for
example – but at many times of day there do seem to be significant
effects of age and gender on the binned-average MIMS values. Finally, we
note that the confidence bands are narrowest in the nighttime hours and
widest during the day, which is consistent with the time-varying
distribution of the outcome shown in earlier Figures.</p>
<p>The hour-level analysis is an informative exploratory approach, but
has several limitations. Most obviously, it aggregates data within
prespecified bins, and in doing so loses some of the richness of the
underlying data. That aggregation induces some smoothness by relying on
the underlying temporal structure, but this smoothness is implicit and
dependent on the bins that are chosen – adjacent coefficient estimates
are similar only because the underlying data are similar, and not
because of any specific model element. To emphasize these points, we can
repeat the bin-level analysis using 10-minute and one-minute epochs.
These can be implemented using only slight modifications to the previous
code, in particular by changing in the bin width of the rolling average
and the grid of argument values over which functions are observed. We
therefore omit this code and focus on the results produced in these
settings.</p>
<pre><code>## setting fill = &#39;extend&#39; for start/end values.
## Warning: There was 1 warning in `mutate()`.
## ℹ In argument: `MIMS_ten = tf_smooth(MIMS_tf, method = &quot;rollmean&quot;, k = 10,
##   align = &quot;center&quot;)`.
## Caused by warning in `tf_smooth.tfd()`:
## ! non-equidistant arg-values in &#39;MIMS_tf&#39; ignored by rollmean.</code></pre>
<p>Results for pointwise regression analyses using 10-minute epochs are
below.</p>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-16-1.png" width="90%" /></p>
<p>Results for one-minute epochs are in the next figure.</p>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-18-1.png" width="90%" /></p>
<p>In each of these analyses, the qualitative results and their
interpretations are similar – the intercept suggests a usual circadian
rhythm, and the age indicates that activity generally decreases with
increasing age, and the gender effect shows that female participants
were more active than male participants. But this also shows some
difficulties inherent in binning-based analyses. These do not leverage
the temporal structure directly in the estimation process. A correct
approach to inference is not obvious, both because of the need for some
degree of smoothness across time and also because within-subject
residuals will have a temporal correlation structure. A more subtly
issue is that we implicitly rely on curves being observed over the same
grid, or at least that a rolling mean is a plausible way to generate
binned averages, and this approach may not work when data are sparse or
irregular across subjects.</p>
<p>Solutions to these problems are the goal of various approaches to
function-on-scalar regression, which seeks to explicitly model
coefficients in a way that induces smoothness; in some way accounts for
within-curve correlation; and extends to a variety of data generating
scenarios.</p>
</div>
</div>
<div id="linear-function-on-scalar-regression" class="section level2">
<h2>Linear Function-on-Scalar Regression</h2>
<p>Let <span class="math inline">\(y_i:\mathcal{S}\rightarrow
\mathbb{R}\)</span> be a functional response of interest for each study
participant, <span class="math inline">\(i=1,\ldots,n\)</span>, and
<span class="math inline">\(x_{i1}\)</span> and <span
class="math inline">\(x_{i2}\)</span> be two scalar predictors. In the
setting considered in this Chapter, the functional response is the MIMS
profile for each participant, and the scalar predictors of interest are
age in years and a variable indicating whether participant <span
class="math inline">\(i\)</span> is male (<span
class="math inline">\(x_{i2} = 0\)</span>) or female (<span
class="math inline">\(x_{i2} = 1\)</span>). The linear
function-on-scalar regression for this setting is <span
class="math display">\[E[Y_i(s)] = \beta_0(s) + \beta_1(s) x_{i1} +
\beta_2(s) x_{i2}\]</span> with coefficients <span
class="math inline">\(\beta_p:\mathcal{S}\rightarrow
\mathbb{R}\)</span>, <span class="math inline">\(p \in \{0, 1,
2\}\)</span> that are functions measured over the same domain as the
response. Scalar covariates in this model can exhibit the same degree of
complexity as in non-functional regressions, allowing any number of
continuous and categorical predictors of interest.</p>
<p>Coefficient functions encode the varying association between the
response and predictors, and are interpretable in ways that parallel
non-functional regression models. In particular, <span
class="math inline">\(\beta_0(s)\)</span> is the expected response for
<span class="math inline">\(x_{i1} = x_{i2} = 0\)</span>; <span
class="math inline">\(\beta_1(s)\)</span> is the expected change in the
response for each one unit change in <span
class="math inline">\(x_{i1}\)</span> while holding <span
class="math inline">\(x_{i2}\)</span> constant; and so on. These are
often interpreted at specific values of <span class="math inline">\(s
\in \mathcal{S}\)</span> to gain intuition for the associations of
interest. In the NHANES data considered so far, for example,
coefficients functions can be used to compare the effect of increasing
age in the morning and evening, keeping gender fixed. Residuals in this
model will contain deviations between the observed responses and the
expected values based on the linear predictor. These can be complex and
their impact on estimation and inference will be discussed in
SUBSECTION.</p>
<p>The linear FoSR model addresses the concerns our exploratory analysis
raised. Because coefficients are functions observed on <span
class="math inline">\(\mathcal{S}\)</span>, they can be estimated using
techniques that explicitly allow for smoothness across the functional
domain. This smoothness, along with appropriate error correlation
structures, provides an avenue for correct statistical inference.
Considering coefficients as functions also opens the door to data
generating mechanisms for observed data that would be challenging or
impossible in more exploratory settings, such as responses that are
observed on grids that are sparse or irregular across subjects.</p>
<div id="estimation-of-fixed-effects" class="section level3">
<h3>Estimation of Fixed Effects</h3>
<p>Functional responses are observed over a discrete grid <span
class="math inline">\(\mathbf{s} = \{s_1, ..., s_J\}\)</span> which, for
now, we will assume to be common across subjects so that for subject
<span class="math inline">\(i\)</span> the observed data vector is <span
class="math inline">\(\mathbf{y}_i^{T} = [y_i(s_1), ...,
y_i(s_J)]\)</span>. From the FoSR model ZZZZ, we have <span
class="math display">\[E[\mathbf{y}_i^{T}] =
\mathbf{x}_i \begin{bmatrix}
\beta_{0}(s_1) &amp; ... &amp; \beta_{0}(s_J)\\
\beta_{1}(s_1) &amp; ... &amp; \beta_{2}(s_J)\\
\beta_{2}(s_1) &amp; ... &amp; \beta_{3}(s_J)
\end{bmatrix} \]</span> where <span class="math inline">\(\mathbf{x}_i =
[1, x_{i1}, x_{i2}]\)</span> is the row vector containing scalar terms
that defines the regression model. This expression is useful because it
connects a <span class="math inline">\(1 \times J\)</span> response
vector to a recognizable row in a standard regression design matrix and
the matrix of functional coefficients.</p>
<p>Estimation of coefficients will rely on approaches that have been
used elsewhere with nuances specific to the FoSR setting. As a starting
point, we will expand each <span class="math inline">\(\beta_p(s) =
\sum_{k=1}^K\beta_{pk} B_k(s)\)</span> using the basis <span
class="math inline">\(B_1(s),\ldots,B_K(s)\)</span>. While many choices
are possible, we will use a spline expansion. Using this, the <span
class="math inline">\(3 \times J\)</span> matrix of coefficient
functions in ZZZZ can be expressed using <span class="math display">\[
\begin{bmatrix}
\beta_{11} &amp; \beta_{21} &amp; \beta_{31}\\
\vdots     &amp; \vdots     &amp; \vdots    \\
\beta_{1K} &amp; \beta_{2K} &amp; \beta_{3K}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{B}_{1}(s_1) &amp; ... &amp; \boldsymbol{B}_{K}(s_1)\\
\vdots      &amp;     &amp; \vdots     \\
\boldsymbol{B}_{1}(s_J) &amp; ... &amp; \boldsymbol{B}_{K}(s_J)
\end{bmatrix}.
\]</span></p>
<p>Conveniently, one can concisely combine and rewrite the previous
expressions. Let <span class="math inline">\(\boldsymbol{B}(s_{j}) =
[B_1(s_{j}), \ldots, B_K(s_{j})]\)</span> be the <span
class="math inline">\(1 \times K\)</span> row vector containing the
basis functions evaluated at <span class="math inline">\(s_{j}\)</span>
and <span class="math inline">\(\boldsymbol{B}(\mathbf{s})\)</span> be
the <span class="math inline">\(K \times J\)</span> matrix containing
basis functions evaluated over the grid <span
class="math inline">\(\mathbf{s}\)</span>. Further, let <span
class="math inline">\(\mathbf{\beta}_{p} = [\beta_{p1}, \ldots,
\beta_{pK}]^{T}\)</span> be the <span class="math inline">\(K \times
1\)</span> vector of basis coefficients for function <span
class="math inline">\(p\)</span> and <span
class="math inline">\(\boldsymbol{\beta} = [\mathbf{\beta}^{T}_{0},
\mathbf{\beta}^{T}_{1}, \mathbf{\beta}^{T}_{2}]^{T}\)</span> be the
<span class="math inline">\((3 \cdot K) \times 1\)</span> vector
constructed by stacking the vectors of basis coefficients. For the <span
class="math inline">\(J \times 1\)</span> response vector <span
class="math inline">\(\mathbf{y}_i\)</span>, we have <span
class="math display">\[
E[\mathbf{y}_i] = [\mathbf{x}_i \otimes \boldsymbol{B}(\mathbf{s})]
\boldsymbol{\beta}
\]</span> where <span class="math inline">\(\otimes\)</span> is the
Kronecker product.</p>
<p>This expression for a single subject can be directly extended to all
subjects. Let <span class="math inline">\(\mathbf{y}\)</span> be the
<span class="math inline">\((n \cdot J) \times 1\)</span> vector created
by stacking the vectors <span
class="math inline">\(\mathbf{y}_i\)</span> and <span
class="math inline">\(\boldsymbol{X}\)</span> be <span
class="math inline">\(n \times 3\)</span> matrix created by stacking the
vectors <span class="math inline">\(\mathbf{x}_i\)</span>. Then <span
class="math display">\[
E[\mathbf{y}] = [\boldsymbol{X} \otimes \boldsymbol{B}(\mathbf{s})]
\boldsymbol{\beta}.
\]</span> This formulation, while somewhat confusing at first, underlies
many implementations of the linear FoSR model. First, we note that the
matrix <span class="math inline">\(\boldsymbol{X}\)</span> is the design
matrix familiar from non-functional regression models, and includes a
row for each subject and a column for each predictor. The Kronecker
product with the basis <span
class="math inline">\(\boldsymbol{B}(\mathbf{s})\)</span> replicates
that basis once for each coefficient function; this product also ensures
that each coefficient function, evaluated over <span
class="math inline">\(\mathbf{s}\)</span>, is multiplied by the
subject-specific covariate vector <span
class="math inline">\(\mathbf{x}_i\)</span>. Finally, considering <span
class="math inline">\([\boldsymbol{B}(\mathbf{s}) \otimes
\boldsymbol{X}]\)</span> as the full FoSR design matrix and <span
class="math inline">\(\boldsymbol{\beta}\)</span> as a vector of
coefficients to be estimated explicitly connects the FoSR model to
standard regression techniques.</p>
<div id="estimation-using-ordinary-least-squares"
class="section level4">
<h4>Estimation using ordinary least squares</h4>
<p>Estimation the spline coefficients, and therefore the coefficient
functions, using ZZZ helps to build intuition for the inner workings of
more complex estimation strategies. We will regress the daily MIMS data
on age and gender to illustrate this approach. Recall that
<code>nhanes_df</code> stores <code>MIMS</code> as a <code>tf</code>
vector; some of the code below relies on extracting information from a
<code>tf</code> object.</p>
<p>For this model, the code chunk below defines the standard design
matrix <span class="math inline">\(\boldsymbol{X}\)</span> and the
response vector <span class="math inline">\(\mathbf{y}\)</span>. The
call to <code>model.matrix()</code> uses a formula that correctly
specifies the predictors, and obtaining the response vector
<code>y</code> unnests the <code>tf</code> vector and then extracts the
observed responses for each subject.</p>
<pre class="r"><code>X_des = 
  model.matrix(
    SEQN ~ gender + age,
    data = nhanes_df
  )

y = 
  nhanes_df %&gt;% 
  tf_unnest(MIMS_tf) %&gt;% 
  pull(MIMS_tf_value)</code></pre>
<p>The next code chunk constructs the spline basis matrix <span
class="math inline">\(\boldsymbol{B}(\mathbf{s})\)</span> and the
Kronecker product <span
class="math inline">\([\boldsymbol{B}(\mathbf{s}) \otimes
\boldsymbol{X}]\)</span>. The spline basis is evaluated over the grid
<span class="math inline">\(\mathbf{s}\)</span> pulled from
<code>MIMS</code>, and uses <span class="math inline">\(K = 30\)</span>
basis functions.</p>
<pre class="r"><code>epoch_arg = seq(1/60, 24, length = 1440)

basis = 
  splines::bs(epoch_arg, df = 30, intercept = TRUE)

X_kron_B = kronecker(X_des, basis)</code></pre>
<p>Using these, the direct application of usual formulas provides OLS
estimates of the spline coefficients. Reconstructing coefficient
functions is possible using ZZZ above, and the results can be converted
to a <code>tf</code> vector for plotting and manipulation. The code
chunk below estimates and plots coefficient functions.</p>
<pre class="r"><code>spline_coefs = solve(t(X_kron_B) %*% X_kron_B) %*% t(X_kron_B) %*% y

OLS_coef_df = 
  tibble(
    method = &quot;OLS&quot;,
    term = colnames(X_des),
    coef = 
      tfd(
        t(basis %*% matrix(spline_coefs, nrow = 30)),
        arg = epoch_arg)
  )

OLS_coef_df %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti(size = 1.1, alpha = .75) +  
  facet_wrap(&quot;term&quot;, scales = &quot;free&quot;)
## Warning in geom_spaghetti(size = 1.1, alpha = 0.75): Ignoring unknown
## parameters: `size`</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-21-1.png" width="90%" /></p>
<pre><code>## Warning in geom_spaghetti(size = 1.1, alpha = 0.75): Ignoring unknown
## parameters: `size`</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-22-1.png" width="90%" /></p>
<p>Comparing these results to those obtained from epoch-specific
regressions begins to indicate the benefits of a functional perspective.
Coefficient functions have a smoothness determined by the underlying
spline expansion, and estimates directly information borrow across
adjacent time points. Our specification relied on the same observation
grid, but this can be relaxed: if data contain subject-specific grids
<span class="math inline">\(\mathbf{s}_i\)</span>, one can replace the
matrix <span class="math inline">\([\boldsymbol{B}(\mathbf{s}) \otimes
\boldsymbol{X}]\)</span> with one constructed by row-stacking matrices
<span class="math inline">\([\mathbf{x}_i \otimes
\boldsymbol{B}(\mathbf{s}_i)]\)</span> (note that it can take some care
to ensure the basis is constant across subjects, for example by
explicitly defining knot points). We will next incorporate smoothness
penalties and account for complex error correlations, but it is helpful
to view these as variations on a familiar regression framework.</p>
</div>
<div id="estimation-using-smoothness-penalties" class="section level4">
<h4>Estimation using smoothness penalties</h4>
<p>Spline expansions for coefficient functions in the linear FoSR model
makes the functional nature of the observed data and the model
parameters explicit, and distinguishes this approach from multivariate
or epoch-based alternatives. However, as we’ve seen elsewhere, using a
spline expansion introduces questions about how best to balance
flexibility with goodness-of-fit. While one could address this through a
careful selection of the number of basis functions <span
class="math inline">\(K\)</span>, it is more common to use a large value
for <span class="math inline">\(K\)</span> and introduce
smoothness-enforcing penalties to prevent overfitting.</p>
<p>Solutions obtained in Section ZZZZ using OLS also arise using maximum
likelihood estimation. Assume that residuals <span
class="math inline">\(\mathbf{\epsilon}_i^{T} = [\epsilon_i(s_1), ...,
\epsilon_i(s_J)]\)</span> observed on <span
class="math inline">\(\mathbf{s}\)</span> are <span
class="math inline">\(\mathrm{iid}\)</span> are drawn from a mean-zero
normal distribution with variance <span
class="math inline">\(\sigma^{2}_{\epsilon}\)</span> that is constant
over time points <span class="math inline">\(\mathbf{s}\)</span> and
subjects <span class="math inline">\(i\)</span>. Maximizing the (log)
likelihood <span class="math inline">\(\mathcal{L}(\boldsymbol{\beta};
\mathbf{y})\)</span> induced by this assumption with respect to spline
coefficients is equivalent to the OLS approach. We can add penalties
<span class="math inline">\(P\left[\beta_{p}(s) \right]\)</span> to
enforce smoothness in estimates of coefficient functions; a common
choice is the second derivative penalty <span
class="math inline">\(\int_{\mathcal{S}} \left[\beta_{p}^{&#39;&#39;}(s)
\right]^2 ds\)</span>. The second derivative penalty and many others can
be expressed in terms of the corresponding coefficient vector, resulting
in the penalized likelihood <span class="math display">\[
\mathcal{L}(\boldsymbol{\beta}; \mathbf{y}) + \sum_{p} \lambda_{p}
P\left[\mathbf{\beta}_{p} \right]\]</span> where tuning parameters <span
class="math inline">\(\lambda_{p}\)</span> control the balance between
goodness-of-fit and complexity of the coefficient functions <span
class="math inline">\(\beta_{p}(s)\)</span>.</p>
<p>Here we are emphasizing a penalized likelihood approach, rather than
using the penalized residual sum of squares, even though both are
possible for the setting we currently consider. Focusing on the
penalized likelihood makes the extension to non-continuous response
functions (e.g. binary rest-active trajectories instead of MIMS
profiles) relatively direct by changing the assumed outcome
distribution, and will similarly facilitate modeling and including error
correlation structures. More subtly, it is possible to view the
likelihood-based framework as a variation on mixed-model approaches to
penalized spline smooth smoothing introduced in Chapter 2.2.3, a
connection that underlies several implementations of the FoSR model.</p>
<p>We will use the <code>gam</code> function in the well-developed
<code>mgcv</code> package to fit the FoSR model with smoothness
penalties. First, we explicitly create the binary indicator variable for
<code>gender</code>, and organize data into “long” format by unnesting
the MIMS data stored as a <code>tf</code> object. The resulting
dataframe has a row for each subject and epoch, containing the
<code>MIMS</code> outcome in that epoch as well as the scalar covariates
of interest. This is analogous to the creation of the response vector
for model fitting using OLS, and begins to organize covariates for
inclusion in a design matrix.</p>
<pre class="r"><code>nhanes_for_gam = 
  nhanes_df %&gt;% 
  mutate(gender = as.numeric(gender == &quot;Female&quot;)) %&gt;% 
  tf_unnest(MIMS_tf) %&gt;% 
  rename(epoch = MIMS_tf_arg, MIMS = MIMS_tf_value)</code></pre>
<p>With data in this form, we can fit the FoSR model using
<code>mgcv::gam()</code> as follows.</p>
<pre class="r"><code>gam_fit = 
  gam(MIMS ~ s(epoch) + s(epoch, by = gender) + s(epoch, by = age), 
      data = nhanes_for_gam)</code></pre>
<p>Conceptually, this specification indicates that each observed
<code>MIMS</code> value is the combination of three smooth functions of
<code>epoch</code>: an intercept function, and the products of
coefficient functions and scalar covariates <code>gender</code> and
<code>age</code>. Specifically, the first model term
<code>s(epoch)</code> indicates the evaluation of a spline expansion
over values contained in the <code>epoch</code> column of the data frame
<code>nhanes_for_gam</code>. The second and third terms add
<code>by = gender</code> and <code>by = age</code>, respectively, which
also indicate spline expansions over the <code>epoch</code> column but
multiply the result by the corresponding scalar covariates. This process
is analogous to the creation of the design matrix <span
class="math inline">\([\boldsymbol{B}(\mathbf{s}) \otimes
\boldsymbol{X}]\)</span>, although <code>gam()</code>’s function
<code>s()</code> allows users to flexibly specify additional options for
each basis expansion.</p>
<p>In contrast the the previous Section’s parameter estimation using
OLS, smoothness is induced in parameter estimates through explicit
penalization. By default, <code>gam()</code> uses thin-plate splines
with second derivative penalties, and selects tuning parameters for each
coefficient using GCV or REML [cite “thin plate regression
splines”].</p>
<p>The results contained in <code>gam_fit</code> are not directly
comparable to those we’ve seen elsewhere, and extracting coefficient
functions requires some additional work. The <code>predict.gam()</code>
function can be used to return each element of the linear predictor for
a provided data frame. In this case, we hope to return smooth functions
of <code>epoch</code> corresponding to the intercept and coefficient
functions. We therefore create a data frame containing an
<code>epoch</code> column consisting of the unique evaluation points of
the observed functions (i.e. minutes); a column <code>gender</code>, set
to <code>1</code> for all epochs; and a column <code>age</code>, also
set to <code>1</code> for all epochs.</p>
<pre class="r"><code>gam_pred_obj = 
  tibble(
    epoch = epoch_arg,
    gender = 1,
    age = 1, 
  ) %&gt;% 
  predict(gam_fit, newdata = ., type = &quot;terms&quot;)</code></pre>
<p>The result contained in <code>gam_pred_obj</code> is a
<code>1440 x 3</code> matrix, with columns corresponding to <span
class="math inline">\(\beta_0(\mathbf{s})\)</span>, <span
class="math inline">\(1 \cdot \beta_1(\mathbf{s})\)</span>, and <span
class="math inline">\(1 \cdot \beta_2(\mathbf{s})\)</span>. We convert
these to <code>tfd</code> objects in the code below. Note that
<code>gam()</code> includes the overall intercept as a (scalar) fixed
effect, which must be added to the intercept function. With data
structured in this way, we can then plot coefficient functions using
tools seen previously.</p>
<pre class="r"><code>gam_coef_df = 
  tibble(
    method = &quot;GAM&quot;,
    term = c(&quot;(Intercept)&quot;, &quot;genderFemale&quot;, &quot;age&quot;),
    coef = 
      tfd(t(gam_pred_obj), arg = epoch_arg)) %&gt;% 
  mutate(coef = coef + c(coef(gam_fit)[1], 0, 0))</code></pre>
<pre class="r"><code>gam_coef_df %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti(size = 1.1, alpha = .75) +  
  facet_wrap(vars(term), scales = &quot;free&quot;) 
## Warning in geom_spaghetti(size = 1.1, alpha = 0.75): Ignoring unknown
## parameters: `size`</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-27-1.png" width="90%" /></p>
<pre><code>## Warning in geom_spaghetti(size = 1.1, alpha = 0.75): Ignoring unknown
## parameters: `size`</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-28-1.png" width="90%" /></p>
<p>The plot below includes estimates obtained from the three approaches
seen so far, and facilitates a comparison of methods. Each approach –
separate epoch-level regressions, FoSR using OLS to estimate spline
coefficients, and FoSR implemented with smoothness penalties in
<code>mgcv::gam()</code> – yields qualitatively similar results
regarding the effect of age and gender on diurnal MIMS trajectories.
This suggests that all approaches can be used at least in exploratory
analyses or to understand general patterns. That said, there are obvious
differences. The epoch-level regressions do not borrow information
across adjacent time points, and the OLS is sensitive to dimension of
the basis expansion in the model specification; both are wigglier, and
perhaps less plausible, than the method that includes smoothness
penalties. As a result, methods that explicitly borrow information
across time and implement smoothness penalties with data-driven tuning
parameters are often preferred for formal analyses.</p>
<pre class="r"><code>ggp_coefplot_compare =
  ggplot(mapping = aes(y = coef, color = method)) + 
  geom_spaghetti(data = min_regressions, alpha = .5) +
  geom_spaghetti(data = OLS_coef_df, size = 1.2) +
  geom_spaghetti(data = gam_coef_df, size = 1.2) +
  facet_wrap(vars(term), scales = &quot;free&quot;) + 
  scale_x_continuous(
    breaks = seq(0, 24, length = 5),
    labels = str_c(seq(0, 24, length = 5), &quot;:00&quot;)) + 
  labs(x = &quot;Time of day (hours)&quot;, y = &quot;Coefficient&quot;)
## Warning in geom_spaghetti(data = OLS_coef_df, size = 1.2): Ignoring unknown
## parameters: `size`
## Warning in geom_spaghetti(data = gam_coef_df, size = 1.2): Ignoring unknown
## parameters: `size`

ggp_coefplot_compare</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-29-1.png" width="90%" /></p>
<pre class="r"><code>
ggsave(
  here::here(&quot;figures&quot;, &quot;ch05_fosr&quot;, &quot;coefplot_compare.pdf&quot;),
  plot = ggp_coefplot_compare,
  width = 8, height = 4.5)</code></pre>
</div>
</div>
<div id="error-correlation-and-inference" class="section level3">
<h3>Error correlation and inference</h3>
<p>To this point, we have developed tools for estimating coefficient
functions in model ZZZ while setting aside concerns about residual
correlation. Of course, in most FoSR settings residuals are indeed
correlated; in our example, MIMS values at 10:00am are informative for
values at 1:00pm.</p>
<p>Connecting FoSR with techniques for FPCA presented in Chapter 3 is an
important approach to modeling residual correlation. In FPCA, the goal
is to separate (mean-zero) functions <span
class="math inline">\(w_i(s)\)</span> into <span
class="math inline">\(f_i(s)\)</span> and <span
class="math inline">\(epsilon_{i}(s)\)</span> under the assumption that
the <span class="math inline">\(f_{i}(s)\)</span> can be decomposed
using a low dimensional orthonormal basis. Chapter 3 often assumed a
population-level mean shared across subjects; here, we will model the
conditional mean using subject-level covariates and decompose residual
variation using FPCA. That is, in our example with two scalar covariates
<span class="math inline">\(x_{i1}\)</span> and <span
class="math inline">\(x_{i2}\)</span> we will model <span
class="math display">\[Y_i(s) = \beta_0(s) + \beta_1(s) x_{i1} +
\beta_2(s) x_{i2} + f_i(s) +\epsilon_i(s)\]</span> using techniques
based in FPCA to model the functions <span
class="math inline">\(f_i(s)\)</span>.</p>
<p>Appropriately accounting for residual correlation affects both
estimation and inference for coefficient function <span
class="math inline">\(\beta_{p}(s)\)</span>. In Section ZZZZ, we’ll use
visual inspection of estimated coefficient functions and their
confidence bands to build intuition for the impact of various approaches
to modeling residual correlation. Our approach to constructing pointwise
confidence intervals is rests on spline-based estimation approaches
considered so far, as well as the assumption that spline coefficient
estimates have an approximately Normal distribution. Treating any tuning
parameters as fixed, for any <span class="math inline">\(s \in
\mathcal{S}\)</span> the variance of <span
class="math inline">\(\hat{\beta}_p(s)\)</span> is given by <span
class="math display">\[\mbox{Var}[\hat{\beta}_p(s)] = \boldsymbol{B}(s)
\mbox{Var}(\mathbf{\beta}_{p}) \boldsymbol{B}(s)^{T} \]</span> From
this, one can obtain standard errors and construct a confidence interval
for <span class="math inline">\(\beta_p(s)\)</span> using the assumption
of Normality.</p>
<p>In keeping with the philosophy of this book, models in this section
will be fit with user-friendly functions that mask a some degree of
underlying complexity. However, we emphasize that these model fitting
strategies are grounded in familiar regression techniques: coefficient
functions are estimated using penalized splines, and the conceptual
framework for modeling residual correlation is based on FPCA and related
to similar structures in longitudinal data analysis.</p>
<div id="modeling-residuals-with-fpca-and-gam" class="section level4">
<h4>Modeling residuals with FPCA and <code>gam</code></h4>
<p>Combining FoSR with FPCA for correlated residuals suggests the
following model structure: <span class="math display">\[Y_i(s) =
\beta_0(s) + \beta_1(s) x_{i1} + \beta_2(s) x_{i2} + \sum_{k = 1}^{K}
\xi_{ik}\phi_k(s) + \epsilon_{i}(s)\]</span> <span
class="math display">\[\xi_{ik} \sim [0, \lambda_k]; \epsilon_i(s) \sim
N[0, \sigma^2].\]</span> As before, functions are observed over discrete
values <span class="math inline">\(\mathbf{s}_i = \{s_1, ...,
s_{J_i}\}\)</span> that can be dense or sparse, regular or irregular,
and shared across subjects or not.</p>
<p>Our approach to estimating coefficient functions <span
class="math inline">\(\beta_0(s)\)</span> will remain essentially
unchanged from previous sections. The challenge now is how best to
estimate terms in the sum <span class="math inline">\(\sum_{k = 1}^{K}
\xi_{ik}\phi_k(s)\)</span>, keeping in mind that both subject level
scores <span class="math inline">\(\xi_{ik}\)</span> and shared
directions of variation <span class="math inline">\(\phi_k(s)\)</span>
are unknown. A key observation in Chapter 3 is that if the <span
class="math inline">\(\phi_k(s)\)</span> are known or if estimates <span
class="math inline">\(\hat{\phi}_k(s)\)</span> are available, then
scores can be estimated as random effects in a mixed model. Our approach
to fitting model ZZZ will be to obtain estimates of the <span
class="math inline">\(\phi_k(s)\)</span>, so that coefficient functions
<span class="math inline">\(\beta_{p}(s)\)</span> and scores <span
class="math inline">\(\xi_{ik}\)</span> can be simultaneously estimated
using penalized splines and random effects, respectively. Although it is
not strictly necessary to fit and interpret the results of a FoSR model,
recognizing that mixed models underlie the penalized spline estimation
for coefficient functions as well as the random effects estimation for
scores connects these two model components and facilitates
implementation.</p>
<p>First, we will assume that <span class="math inline">\(K = 1\)</span>
and <span class="math inline">\(\phi_{1}(s) = 1\)</span> for all <span
class="math inline">\(s \in \mathcal{S}\)</span>. This effectively adds
a subject-level random intercept to the FoSR model, and is a useful
contrast between longitudinal data analysis and functional data
analysis: the former typically makes assumptions that limit the
flexibility of subject-level estimates over the observation interval
<span class="math inline">\(\mathcal{S}\)</span>, while the latter uses
data-driven approaches to add flexibility where appropriate.</p>
<p>To fit the random intercept model, we adapt our previous
implementation for penalized spline estimation in a number of ways.
Recall that <code>nhanes_for_gam</code> contains a long-form dataframe
with rows for each subject and epoch, and columns containing
<code>MIMS</code> and scalar covariates. This dataframe also contains a
column of subject IDs <code>SEQN</code>; importantly, this is encoded as
a factor variable and therefore can be used to define subjects in the
following a random effects specification. In the formula component of
the model below, the terms corresponding to fixed effects are unchanged,
but we add a term <code>s(SEQN)</code> with the argument
<code>bs = "re"</code>. This creates a “smooth” term with a random
effects “basis” – essentially taking advantage of the noted connection
between semiparametric regression and random effects estimation to
obtain subject-level random effects estimates and the corresponding
variance component. Finally, we note that we use <code>bam</code>
instead of <code>gam</code>, and add arguments
<code>method = "fREML"</code> and <code>discrete = TRUE</code>. These
changes dramatically decrease computation times.</p>
<pre class="r"><code>nhanes_gamm_ranint = 
  nhanes_for_gam %&gt;% 
  bam(MIMS ~ s(epoch) + s(epoch, by = gender) + s(epoch, by = age) + 
             s(SEQN, bs = &quot;re&quot;),
      method = &quot;fREML&quot;, discrete = TRUE, data = .)</code></pre>
<p>Next, we will use a two-step approach to model correlated residuals
using FPCA. The first step is to fit a FoSR model assuming uncorrelated
errors to obtain estimates of fixed effects <span
class="math inline">\(\beta_{p}(s)\)</span> and fitted values <span
class="math inline">\(\hat{Y_i}(s)\)</span> for each subject. From
these, we compute residuals <span class="math inline">\(\hat{w}_i(s) =
Y_{i}(s) = \hat{Y_i}(s)\)</span> which can be decomposed using standard
FPCA methods to produce estimates of FPCs <span
class="math inline">\(\hat{\phi}_{k}(s)\)</span>. In the second step,
the <span class="math inline">\(\hat{\phi}_{k}(s)\)</span> are treated
as “known” in model ZZZZ; coefficient functions <span
class="math inline">\(\beta_{p}(s)\)</span> and subject-level scores
<span class="math inline">\(\xi_{ik}\)</span> are then estimated
simultaneously. The NHANES data considered in this Chapter are densely
measured over a regular grid that is common to all subjects and we
implement the FPCA step using the FACE method, but the same two-step
approach can be adapted to other data settings with appropriate model
choices.</p>
<p>In the code below, we first construct a dataframe that contains
fitted values and residuals, and can subsequently be used as the basis
for FPCA. We start with <code>nhanes_for_gam</code>, and add fitted
values from our penalized spline estimation assuming independent errors
in <code>gam_fit</code>; residuals are created by subtracting the fitted
values from the <code>MIMS</code> observations. After keeping only the
subject ID <code>SEQN</code>, <code>epoch</code> and the residual values
for each subject at each epoch, we nest to create a dataframe in which
residual curves are <code>tf</code> values. Using this dataframe, we
conduct FPCA using the <code>rfr_fpca()</code> function in the
<code>refundr</code> R package. At the time of writing,
<code>refundr</code> is under active development and contains
reimplementations of many functions in the <code>refund</code> package
using <code>tidyfun</code> for data organization. By default, for data
observed over a regular grid <code>rfr_fpca()</code> uses FACE to
conduct FPCA. We specify two additional arguments in this call. First,
<code>center == FALSE</code> avoids recomputing the mean function since
we are using residuals from a previous fit. Second, we set the smoothing
parameter by hand using <code>lambda = 50</code>; this produces smoother
FPCs than the default implementation. Of course, the same methodology
can be implemented using <code>refund</code> and
<code>fpca.face()</code> directly, but here we illustrate a functional
<code>tidy</code>-friendly approach.</p>
<pre class="r"><code>nhanes_fpca_df = 
  nhanes_for_gam %&gt;% 
  mutate(
    fitted = fitted(gam_fit),
    resid = MIMS - fitted) %&gt;% 
  select(SEQN, epoch, resid) %&gt;% 
  tf_nest(resid, .id = SEQN, .arg = epoch)

nhanes_resid_fpca = 
  rfr_fpca(&quot;resid&quot;, nhanes_fpca_df, center = FALSE, lambda = 50)
## Warning in new_tfb_fpc(data, domain = domain, method = method, resolution = resolution, : domain for tfb_fpc can&#39;t be larger than observed arg-range -- extrapolating FPCs is a bad idea.
##  domain reset to [0.017,24]</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-32-1.png" width="90%" /></p>
<p>The second step in our two-stage approach is to treat the resulting
FPCs as “known” so that scores can be estimated as random effects. We
will use a variation on the random intercept implementation to do this.
In particular, our goal remains to estimate a random effect or score
<span class="math inline">\(\xi_{ik}\)</span> for each subject and FPC,
but now that score will multiply the FPC evaluated over the functional
domain. Put differently, we want to scale one component in our model by
another; to do this, we will again make use of the <code>by</code>
argument in the <code>s()</code> function.</p>
<p>The code chunk below defines the dataframe necessary to implement
this strategy. It repeats code seen before to convert the
<code>nhanes_df</code> dataframe to a format needed by
<code>mgcv::gam()</code>, by creating an indicator variable for
<code>gender</code> and unnesting the <code>MIMS</code> data stored as a
<code>tf</code> object. However, we also add a column <code>fpc</code>
that contains the first FPC estimated above. The FPC is the same for
each participant and here is treated as a <code>tf</code> object. We
then unnest both <code>MIMS</code> and <code>fpc</code> to produce a
long-format dataframe with row for each subject and epoch.</p>
<pre class="r"><code>nhanes_for_gamm = 
  nhanes_df %&gt;% 
  mutate(
    gender = as.numeric(gender == &quot;Female&quot;),
    fpc = tfd(nhanes_resid_fpca$efunctions[,1])) %&gt;% 
  tf_unnest(cols = c(MIMS_tf, fpc)) %&gt;% 
  rename(epoch = MIMS_tf_arg, MIMS = MIMS_tf_value, fpc = fpc_value) %&gt;% 
  select(-fpc_arg)</code></pre>
<p>With these data organized appropriately, we can estimate coefficient
functions and subject-level FPC scores using a small modification to our
previous random intercept approach. We again estimate subject-level
effects using <code>s(SEQN)</code> and a random effect “basis” that adds
a random intercept for each participant. Including <code>by = fpc</code>
scales the random effect basis by the FPC value in each epoch,
effectively creating the term <span class="math inline">\(\xi_{i1}
\phi_{1}(s)\)</span> by treating <span
class="math inline">\(\phi_{1}(s)\)</span> as known.</p>
<pre class="r"><code>nhanes_gamm_fpc = 
  nhanes_for_gamm %&gt;% 
  bam(MIMS ~ s(epoch, fx = T) + s(epoch, by = gender, fx = T) + s(epoch, by = age, fx = T) + 
             s(SEQN, bs = &quot;re&quot;, by = fpc),
      discrete = TRUE, method = &quot;fREML&quot;, data = .)</code></pre>
<p>This implementation of the FoSR model using FPCA to account for
residual correlation has important strengths. Penalized spline
estimation of fixed effects leverages information across adjacent time
points and enforces smoothness, and residual correlation is accounted
for in a flexible, data-driven way. Some additional refinements are
possible, and may be useful in practice; these include the incorporation
of several FPCs into a term of the form <span
class="math inline">\(\sum_{k = 1}^{K} \xi_{ik}\phi_k(s)\)</span> and
iterating so that FPCs are derived from a model that has accounted for
residual correlation in the estimation of fixed effects.</p>
<!--
#### Modeling residuals with FPCA "by hand"

I don't think this section is necessary but wanted to keep some code:




-->
</div>
<div id="inference-for-coefficient-functions" class="section level4">
<h4>Inference for coefficient functions</h4>
<p>At the beginning of this section, we noted that formulas for <span
class="math inline">\(\mbox{Var}[\hat{\beta}_p(s)]\)</span> and an
assumption that spline coefficients are Normally distributed (at least
for large samples) make it possible to construct pointwise confidence
intervals for coefficient functions. A critical component is the
covariance of estimated spline coefficients <span
class="math inline">\(\mbox{Var}(\mathbf{\beta}_{p})\)</span>, which is
heavily dependent on the modeling assumptions used to estimate spline
coefficients. We have fit three FoSR models using penalized splines for
coefficient functions, with different assumptions about residuals,
namely that: residuals are uncorrelated with a subject; residual
correlation can be modeled using a random intercept; and residual
correlation can be accounted for using FPCA. We now compare the
estimated coefficient functions and pointwise confidence intervals
obtained from these methods.</p>
<p>To extract coefficient functions and their standard errors from the
objects produced by <code>mgcv::gam</code>, we again make use of the
<code>predict()</code> function. This function takes an input dataframe
that has all covariates used in the model; here we will use the first
<code>1440</code> rows of the <code>nhanes_for_gamm</code> dataframe,
which has all epoch-level observations for a single subject. We set
<code>gender</code> and <code>age</code> to 1 as before, so terms
produced by <code>predict()</code> will correspond to coefficient
functions. When calling <code>predict</code>, we now set the argument
<code>se.fit = TRUE</code>, so that both terms and their standard errors
are returned.</p>
<pre class="r"><code>nhanes_fpc_pred_obj = 
  nhanes_for_gamm[1:1440,] %&gt;% 
  mutate(gender = 1, age = 1) %&gt;% 
  predict(
    nhanes_gamm_fpc, newdata = ., 
    type = &quot;terms&quot;, se.fit = TRUE)</code></pre>
<p>The output of <code>predict()</code> requires some processing before
plotting. We construct <code>coef_df</code> to contain the relevant
output, again using steps that are similar to those seen previously: a
<code>term</code> variable is created and coefficients are extracted and
converted to <code>tf</code> objects, and the model’s overall intercept
is added to the intercept function. There are some important changes,
however. Because we set <code>se.fit = TRUE</code> the result contains
both coefficients and standard errors, and we extract these from the
<code>fit</code> and <code>se.fit</code> elements of the object returned
by <code>predict</code>, respectively. The model that uses FPCA to
account for residual correlation also includes a coefficient for
<code>SEQN</code>, and so we use only the first three terms in the
model. Finally, the code also includes a step to obtain upper and lower
bounds of a 95% pointwise confidence interval, constructed by adding and
subtracting 1.96 times the standard error to the estimate for each
coefficient.</p>
<pre class="r"><code>coef_df =
  tibble(
    term = c(&quot;(Intercept)&quot;, &quot;genderFemale&quot;, &quot;age&quot;),
    coef = tfd(t(nhanes_fpc_pred_obj$fit[,1:3]), arg = epoch_arg),
    se = tfd(t(nhanes_fpc_pred_obj$se.fit[,1:3]), arg = epoch_arg)) %&gt;%
  mutate(coef = coef + c(coef(nhanes_gamm_fpc)[1], 0, 0)) %&gt;% 
  mutate(
    ub = coef + 1.96 * se, 
    lb = coef - 1.96 * se)</code></pre>
<p>Although we don’t show all steps here, the same approach can be used
to extract coefficient functions and confidence intervals for the three
approaches to accounting for residual correlation. The results are
combined into <code>comparison_plot_df</code> and plotted in the figure
below. Note the addition of <code>geom_errorband()</code>, which adds
shaded regions according to each coefficient functions upper and lower
confidence limits.</p>
<pre class="r"><code>comparison_plot_df %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti() + 
  geom_errorband(aes(ymax = ub, ymin = lb, fill = term))+ 
  facet_grid(term ~ method, scales = &quot;free&quot;)</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-40-1.png" width="90%" /></p>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-41-1.png" width="90%" /></p>
<p>The coefficient functions obtained by all methods are similar to each
other and to those based on epoch-level regressions. The confidence
bands, meanwhile, differ substantially in a way that is intuitive based
on the assumed error structures. Assuming independence fails to capture
any of the correlation that exists within subjects, and therefore has
overly narrow confidence bands. Using a random intercept accounts for
some of the true correlation but makes restrictive parametric
assumptions on the correlation structure. Because this approach
effectively induces uniform correlation over the domain, the resulting
intervals are wider than those of under the model assuming independence
but have a roughly fixed width over the day. Finally, modeling residual
curves using FPCA produces intervals that are narrower in the nighttime
and wider in the daytime, which more accurately reflects the variability
across subjects in this dataset. This model suggests a significant
decrease in MIMS as age increases over much of the day, and a
significant increase in MIMS comparing women to men in the morning and
afternoon.</p>
<p>In some ways, it is unsurprising that the coefficient function
estimates produced under different assumptions are similar. After all,
each is an unbiased estimator for the fixed effects in the model. But
the complexity of the underlying maximization problem can produce
counter-intuitive results in some cases. In this analysis, the results
of the FPCA model fitting are sensitive to the degree of smoothness in
the FPC. When the less-smooth FPCs produced by the default FACE settings
were used, the coefficient function estimates were somewhat attenuated.
At the same time, the random effects containing FPC scores were
dependent on the scalar covariates in a way that exactly offset this
attenuation. This does not appear to be an issue with the
<code>gam</code> implementation, because “by hand” model fitting showed
the same sensitivity to smoothness in the FPCs. Instead, we believe this
issues stems from subtle identifiability issues and the underlying
complexity of the penalized likelihood.</p>
</div>
</div>
<div id="modeling-residuals-using-splines" class="section level3">
<h3>Modeling residuals using splines</h3>
<p>This code is basically works but isn’t especially complex. The
results closely mimic what’s above in gam, although there are some
differences in the underlying model specifications. Could make this more
of a precise match, but I think that should be driven by what’s needed
in other chapters. Also – this is unexpected a lot slower than gam …</p>
<pre class="r"><code>nhanes_famm_df &lt;-
  nhanes_df %&gt;%
  mutate(
    MIMS_hour_tf = 
      tf_smooth(MIMS_tf, method = &quot;rollmean&quot;, k = 60, align = &quot;center&quot;),
    MIMS_hour_tf = tfd(MIMS_hour_tf, arg = seq(.5, 23.5, by = 1))) %&gt;%
  tf_unnest(MIMS_hour_tf) %&gt;%
  rename(epoch = MIMS_hour_tf_arg, MIMS_hour = MIMS_hour_tf_value) %&gt;%
  pivot_wider(names_from = epoch, values_from = MIMS_hour)
## setting fill = &#39;extend&#39; for start/end values.
## Warning: There was 1 warning in `mutate()`.
## ℹ In argument: `MIMS_hour_tf = tf_smooth(MIMS_tf, method = &quot;rollmean&quot;, k = 60,
##   align = &quot;center&quot;)`.
## Caused by warning in `tf_smooth.tfd()`:
## ! non-equidistant arg-values in &#39;MIMS_tf&#39; ignored by rollmean.

MIMS_hour_mat &lt;-
  nhanes_famm_df %&gt;%
  select(as.character(seq(0.5, 23.5, by = 1))) %&gt;%
  as.matrix()

nhanes_famm_df &lt;-
  nhanes_famm_df %&gt;%
  select(SEQN, gender, age) %&gt;%
  mutate(MIMS_hour_mat = I(MIMS_hour_mat))</code></pre>
<pre class="r"><code>nhanes_famm = 
  nhanes_famm_df %&gt;%
  pffr(MIMS_hour_mat ~ age + gender + s(SEQN, bs = &quot;re&quot;),
       data = ., algorithm = &quot;bam&quot;, discrete = TRUE,
       bs.yindex = list(bs = &quot;ps&quot;, k = 15, m = c(2, 1)))</code></pre>
<pre><code>## using seWithMean for  s(yindex.vec) .
## using seWithMean for  s(yindex.vec):genderMale .
## using seWithMean for  s(yindex.vec):genderFemale .
## Scale for x is already present.
## Adding another scale for x, which will replace the existing scale.</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-44-1.png" width="90%" /></p>
</div>
</div>
<div id="a-scalable-approach-based-on-epoch-level-regressions"
class="section level2">
<h2>A scalable approach based on epoch-level regressions</h2>
<p>FoSR modeling often involves high-dimensional data with resulting
computational pressures. In regressing <code>MIMS</code> values on
<code>age</code> and <code>gender</code>, we have largely avoided a
discussion of these issues by focusing on a subset comprised of 250
subjects. However, it is worth noting the scale of required matrix
computations. For 250 subjects observed over 1440 epochs each, design
matrices will have 360,000 rows; if 30 spline basis functions are used
in the estimation of coefficient functions, this design matrix will have
90 columns. Increasing the number of subjects or predictors will
exacerbate problems of scale. The tools we’ve used so far, in particular
the <code>mgcv</code> package for penalized spline estimate, are
well-developed and computationally efficient, but can nonetheless
struggle to meet the demands of some modern datasets.</p>
<p>A very simple alternative strategy revisits the epoch-level
regressions that previously motivated a switch to “functional”
techniques. Recall that the epoch-level regression models did not
account for temporal structure, but simply fit standard regression
models at each epoch separately. The “functional” approach explicitly
accounted for temporal structure by expanding coefficient functions of
interest and shifting focus to the estimation of spline coefficients.
Instead, one can smooth epoch-level regression coefficients to obtain
estimates of coefficient functions. Computationally, this requires
fitting many simple models rather than one large model, and can scale
easily as the number of subjects or covariates increases.</p>
<p>In Section RegBinData, we saw results of minute-level regression. The
code below implements that analysis, and is similar to the approach seen
for one-hour epochs. In particular, we unnest the <code>MIMS</code>
column containing a <code>tf</code> vector for functional observations,
which produces a long-format dataframe containing all subject- and
epoch-level observations. We then re-nest by <code>epoch</code>, so that
data across all subjects within an epoch are consolidated; this step
allows the epoch-by-epoch regressions of <code>MIMS</code> on
<code>age</code> and <code>gender</code> by mapping across epoch-level
datasets. The results of the regressions are turned into dataframes by
mapping the <code>broom::tidy</code> function across model results.
Finally, epoch-level regression results are unnested so that the
intercept and two regression coefficients are available for each
epoch.</p>
<pre class="r"><code>min_regressions =
  nhanes_df %&gt;%
  select(SEQN, age, gender, MIMS_tf) %&gt;%
  tf_unnest(MIMS_tf) %&gt;%
  rename(epoch = MIMS_tf_arg, MIMS = MIMS_tf_value) %&gt;%
  nest(data = -epoch) %&gt;%
  mutate(
    model = map(.x = data, ~lm(MIMS ~ age + gender, data = .x)),
    result = map(model, broom::tidy)
  ) %&gt;%
  select(epoch, result) %&gt;%
  unnest(result)</code></pre>
<p>The next step in this analysis is to smooth the regression
coefficients across epochs. Many techniques are available for this; the
approach implemented below organizes epoch-level regression coefficients
as <code>tf</code> objects in the a term called <code>coef</code>, and
then smooths the results using a lowess smoother in the
<code>tf_smooth()</code> function. The resulting data frame has three
columns: <code>term</code> (taking values <code>(Intercept)</code>,
<code>age</code> and <code>genderFemale</code>, <code>coef</code>
(containing the unsmoothed results of epoch-level regressions),
<code>smooth_coef</code> (containing the smoothed versions of values in
<code>coef</code>). The figure below contains a panel for each term, and
shows epoch-level and smooth coefficients. Note the similarity between
the smoothed coefficients and those obtained by “functional” approaches,
including penalized splines; this suggests that this technique is a
plausible and scalable approach to FoSR modeling.</p>
<pre class="r"><code>fui_coef_df =
  min_regressions %&gt;%
  select(epoch, term, coef = estimate) %&gt;%
  tf_nest(coef, .id = term, .arg = epoch) %&gt;%
  mutate(smooth_coef = tf_smooth(coef, method = &quot;lowess&quot;))

fui_coef_df %&gt;% 
  ggplot(aes(y = smooth_coef, color = term)) + 
  geom_spaghetti(size = 1.2) + 
  geom_spaghetti(aes(tf = coef), alpha = .2)  + 
  facet_wrap(~term, scales = &quot;free&quot;) 
## Warning in geom_spaghetti(size = 1.2): Ignoring unknown parameters: `size`
## Warning in geom_spaghetti(aes(tf = coef), alpha = 0.2): Ignoring unknown
## aesthetics: tf</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-46-1.png" width="90%" /></p>
<pre><code>## Warning in geom_spaghetti(size = 1.2): Ignoring unknown parameters: `size`
## Warning in geom_spaghetti(aes(tf = coef), alpha = 0.2): Ignoring unknown
## aesthetics: tf</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-47-1.png" width="90%" /></p>
<p>A main focus of Section AccErrCorr was to model error structures and
thereby obtain accurate inference. The scalable approach we suggest in
this section models each epoch separately, but the residual correlation
is implicit: regression coefficients across epochs are related through
the residual covariance. This fact, and the scalability of the
estimation algorithm, suggests that bootstrapping is a plausible
inferential strategy in this setting. In particular, we suggest to:
resample participants, including full response functions, with
replacement to create bootstrap samples; fit epoch-level regressions for
each bootstrap sample and smooth the results; and construct confidence
intervals based on the results. This resampling strategy preserves the
within-subject correlation structure of the full data without making
additional assumptions on the form of that structure. From a
computational perspective, bootstrap always increases computation time
because one needs to refit the same model multiple times. However,
pointwise regression and smoothing is a simple and relatively fast
procedure, which makes the entire process much more computationally
scalable than the joint modeling approaches described in Section
AccErrCorr. Moreover, the approach is easy to parallelize which can
further improve computational times.</p>
<p>Our implementation of this analysis relies on a helper function
{nhanes_boot_fui}, which has arguments {seed} and {df}. This function
contains the following steps: first, it sets the seed to ensure
reproducibility and then creates a bootstrap sample from the provided
dataframe; second, it creates the hat matrix that is shared across all
epoch-level regressions; third, it estimates epoch-level coefficients by
multiplying the hat matrix by the response vector at each epoch; and
fourth, it smooths these coefficients and returns the results. Because
these steps are relatively straightforward, we defer the function to a
supplement. The code chunk below uses {map} and {nhanes_boot_fui} to
obtain results across 250 bootstrap samples. In practice one may need to
run more bootstrap iterations, but this suffices for illustration. After
unnesting, we have smooth coefficients for each iteration.</p>
<pre class="r"><code>fui_bootstrap_results = 
  tibble(iteration = 1:250) %&gt;% 
  mutate(
    boot_res = map(iteration, nhanes_boot_fui, df = nhanes_df)
  ) %&gt;% 
  unnest(boot_res)</code></pre>
<p>Figure ZZZ shows results of this analysis. We include the full-sample
estimates and estimates obtained in 25 bootstrap samples in the
background. We also show pointwise confidence intervals by adding and
subtracting <span class="math inline">\(1.96\)</span> times the
pointwise standard errors obtained via the bootstrap to the full-sample
estimates. Constructing joint confidence intervals requires one to
account for the complete joint distribution of the functions.</p>
<pre><code>## Warning in geom_spaghetti(size = 1.2, alpha = 0.9): Ignoring unknown parameters:
## `size`
## Warning in geom_spaghetti(data = filter(fui_bootstrap_results, iteration &lt;= :
## Ignoring unknown parameters: `size`</code></pre>
<p><img src="chapter_05_files/figure-html/unnamed-chunk-49-1.png" width="90%" /></p>
</div>
<div id="bayes_fosr" class="section level2">
<h2>bayes_fosr()</h2>
<p>just curious if this works, honestly.</p>
<p>and … not enough smoothing by default, with error bounds that are too
narrow. probably won’t include. also i changed variable names and this
code is broken – should be easy to fix, though.</p>
<pre class="r"><code>bayes_fosr_fit = 
  nhanes_for_pffr %&gt;% 
  bayes_fosr(MIMS ~ gender + age, data = ., Kt = 20, Kp = 2, 
             est.method = &quot;VB&quot;, cov.method = &quot;FPCA&quot;)

bayes_fosr_coef_df = 
  tibble(
    term = c(&quot;(Intercept)&quot;, &quot;genderFemale&quot;, &quot;age&quot;),
    coef = tfd(bayes_fosr_fit$beta.hat),
    ub = tfd(bayes_fosr_fit$beta.UB), 
    lb = tfd(bayes_fosr_fit$beta.LB)
  )

bayes_fosr_coef_df %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti() + 
  geom_errorband(aes(ymax = ub, ymin = lb, fill = term)) + 
  facet_wrap(vars(term), scales = &quot;free&quot;)</code></pre>
</div>
<div id="gfam" class="section level2">
<h2>GFAM</h2>
<p>Trying a GFAM ..</p>
<p>(also omitting this for now …)</p>
<pre class="r"><code>pffr_gfam_fit = pffr(MIMS ~ gender + s(age), data = nhanes_famm_df)

pffr_gfam_coef_df = 
  tibble(
    term = c(&quot;(Intercept)&quot;, &quot;genderFemale&quot;, &quot;age&quot;),
    raw_coef = coef(pffr_gfam_fit)$smterms
  ) %&gt;% 
  filter(term != &quot;age&quot;) %&gt;% 
  mutate(
    raw_coef = map(raw_coef, &quot;coef&quot;),
    coef = map(raw_coef, pffrcoef_to_tf)
  ) %&gt;% 
  select(term, coef) %&gt;% 
  unnest(coef) %&gt;% 
  mutate(
    ub = coef + 1.96 * se, 
    lb = coef - 1.96 * se) 

ggp_gfam_linear = 
  pffr_gfam_coef_df %&gt;% 
  ggplot(aes(y = coef, color = term)) + 
  geom_spaghetti() + 
  geom_errorband(aes(ymax = ub, ymin = lb, fill = term))  + 
  facet_wrap(~term, scales = &quot;free&quot;)

ggp_gfam_smooth = 
  coef(pffr_gfam_fit)$smterms$`s(age)`$coef %&gt;% 
  ggplot(aes(x = yindex.vec, y = age, fill = value)) + 
  geom_tile()

ggp_gfam_linear + ggp_gfam_smooth</code></pre>
</div>
<div id="next-steps" class="section level2">
<h2>Next steps</h2>
<ul>
<li>include time varying coefficient models in lit review</li>
<li>(do lit review more generally)</li>
</ul>
<p>need to consider how to do generalized approaches</p>
<p>say something somewhere about irregular or sparse data</p>
<p>say something about sparsity inducing penalties</p>
</div>

<br><br>
<footer>
  <p class="copyright text-muted" align="center">Copyright &copy; 2023</p>
</footer>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
