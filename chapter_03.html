<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Chapter 3: FPCA</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" />
<script defer src="https://use.fontawesome.com/releases/v5.0.3/js/all.js"></script>
<script defer src="https://use.fontawesome.com/releases/v5.0.0/js/v4-shims.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151578452-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-151578452-1');
</script>
-->

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">FDA with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about_authors.html">About the Authors</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Datasets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="dataset_nhanes.html">NHANES</a>
    </li>
    <li>
      <a href="dataset_covid19.html">COVID-19</a>
    </li>
    <li>
      <a href="dataset_cd4.html">CD4</a>
    </li>
    <li>
      <a href="dataset_content.html">Content</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Chapters
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="chapter_01.html">Chapter 1</a>
    </li>
    <li>
      <a href="chapter_02.html">Chapter 2</a>
    </li>
    <li>
      <a href="chapter_03.html">Chapter 3: FPCA</a>
    </li>
    <li>
      <a href="chapter_04.html">Chapter 4: SoFR</a>
    </li>
    <li>
      <a href="chapter_05.html">Chapter 5: FoSR</a>
    </li>
    <li>
      <a href="chapter_06.html">Chapter 6: FoFR</a>
    </li>
    <li>
      <a href="chapter_07.html">Chapter 7</a>
    </li>
    <li>
      <a href="chapter_08.html">Chapter 8</a>
    </li>
    <li>
      <a href="chapter_09.html">Chapter 9</a>
    </li>
  </ul>
</li>
<li>
  <a href="scripts.html">Scripts</a>
</li>
<li>
  <a href="https://github.com/FDAwithR">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Chapter 3: FPCA</h1>

</div>


<div id="defining-fpca-and-connections-to-pca" class="section level2">
<h2>Defining FPCA and Connections to PCA</h2>
<p>This section illustrates functional principal component analysis
(fPCA) using both simulations. It is associated with Chapter 3 of the
book Functional Data Analysis with R. We use dense and dense with
missing observations data structures.</p>
<p>Functional PCA is closely related to multivariate PCA, but uses the
ordering of the observed data and smoothness of the underlying signal to
reduce the rank of the approximation and increase interpretability of
results. The idea is to find a small set of orthonormal functions that
explain most of the variability of the observed signal.</p>
</div>
<div id="simulations" class="section level2">
<h2>Simulations</h2>
<div id="dense-single-level-functional-data" class="section level3">
<h3>Dense single-level functional data</h3>
<p>We start by simulating dense functional data from a set of
orthonormal functions. In practice these functions are unknown and would
need to be estimated from the data. In simulations the orthonormal
functions are known. All functions are generated on an equally spaced
grid between <span class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span> from the model <span
class="math display">\[W_i(t)=\sum_{k=1}^K\xi_{ik}\phi_k(t)+\epsilon_{i}(t)\;,\]</span>
where <span class="math inline">\(K=4\)</span>, <span
class="math inline">\(\xi_{ik}\sim N(0,\lambda_K)\)</span>, <span
class="math inline">\(\epsilon_{i}(t)\sim N(0,\sigma^2)\)</span>, <span
class="math inline">\(\xi_{ik}\)</span> and <span
class="math inline">\(\epsilon_i(t)\)</span> are mutually independent
for all <span class="math inline">\(i\)</span>, <span
class="math inline">\(k\)</span>, and <span
class="math inline">\(t\)</span>. For illustration purposes we set <span
class="math inline">\(\lambda_k=0.5^{k-1}\)</span> for <span
class="math inline">\(k=1,\ldots,4\)</span> and <span
class="math inline">\(\sigma=2\)</span>, which corresponds to high
noise. The number of study participants is set to <span
class="math inline">\(n=50\)</span> and the number grid points is set to
<span class="math inline">\(p=3000\)</span> to illustrate a case of high
dimensional data. We start by simulating data that are completely
observed for every study participant. We use the Fourier orthonormal
functions <span class="math inline">\(\phi_k(t)\)</span>:<span
class="math display">\[\phi_1(t)=\sqrt{2}\sin(2\pi t); \;
\phi_2(t)=\sqrt{2}\cos(2\pi t);\; \phi_3(t)=\sqrt{2}\sin(4\pi t);\;
\phi_4(t)=\sqrt{2}\cos(4\pi t)\]</span></p>
<p>Below we display the R code for simulating the data.</p>
<pre class="r"><code>set.seed(5242022)
#### settings (I--&gt;n,J--&gt;p,N--&gt;K)
n &lt;- 50 # number of subjects
p &lt;- 3000 # dimension of the data
t &lt;- (1:p) / p # a regular grid on [0,1]
K &lt;- 4 #number of eigenfunctions
sigma &lt;- 2 ##standard deviation of the random noise
lambdaTrue &lt;- c(1, 0.5, 0.5 ^ 2, 0.5 ^ 3) # True eigenvalues
  
# True eigenfunctions stored in a p by K dimensional matrix
#Each column corresponds to one eigenfunction  
phi &lt;- sqrt(2) * cbind(sin(2 * pi * t), cos(2 * pi * t), 
                       sin(4 * pi * t), cos(4 * pi * t))</code></pre>
<p>Note that the functions <span
class="math inline">\(\phi_k(\cdot)\)</span> are orthonormal in <span
class="math inline">\(L_2\)</span>. However, we cannot work directly
with the functions and, instead, we work with a vector of observations
along the functions <span
class="math inline">\(\phi_k=\{\phi_k(t_1),\ldots,\phi_k(t_p)\}\)</span>,
which is in <span class="math inline">\({\mathcal{R}^p}\neq
L_2\)</span>. Even though we have started with orthonormal functions in
<span class="math inline">\(L_2\)</span> these vectors are not
orthonormal in <span class="math inline">\(\mathcal{R}^p\)</span>.
Indeed, they need to be normalized by <span
class="math inline">\(1/\sqrt{p}\)</span> and the cross products are
close to, but not exactly, zero because of numerical approximations.
However, after normalization the approximation to orthonormality in
<span class="math inline">\({\mathcal{R}^p}\)</span> is very good</p>
<pre class="r"><code>round(t(phi) %*% phi / p, digits = 5)
##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    1    0    0
## [3,]    0    0    1    0
## [4,]    0    0    0    1</code></pre>
<p>To better explain this consider the <span
class="math inline">\(L_2\)</span> norm of any of the functions. It can
be shown that <span
class="math inline">\(\int_0^1\phi_k^2(t)dt=1\)</span> for every <span
class="math inline">\(k\)</span>, which indicates that the function has
norm <span class="math inline">\(1\)</span> in <span
class="math inline">\(L_2\)</span>. The integral can be approximated by
the Riemann sum <span
class="math display">\[1=\int_0^1\phi_k^2(t)dt\approx \sum_{j=1}^p
(t_j-t_{j-1})\phi_k^2(t_j)=\frac{1}{p}\sum_{j=1}^p\phi_k^2(t_j)=\frac{1}{p}||\phi_k||^2=\{\phi_k/\sqrt{p}\}^t\{\phi_k/\sqrt{p}\}\;.\]</span>
Here <span class="math inline">\(t_j=j/p\)</span> for <span
class="math inline">\(j=0,\ldots,p\)</span> and <span
class="math inline">\(||\phi_k||^2=\sum_{j=1}^p\phi_k^2(t_j)\)</span> is
the <span class="math inline">\(L_2\)</span> norm of the vector <span
class="math inline">\(\{\phi_k(t_1),\ldots,\phi_k(t_p)\}^t\)</span> in
<span class="math inline">\(\mathcal{R}^p\)</span>. This is different,
though closely related to, the <span class="math inline">\(L_2\)</span>
norm in the functional space on <span
class="math inline">\([0,1]\)</span>. The norm in the vector space is
the Riemann sum approximation to the integral of the square of the
function. The two have the same interpretation when the observed vectors
are divided by <span class="math inline">\(\sqrt{p}\)</span> (when data
are equally spaced and the distance between grid points is <span
class="math inline">\(1/p\)</span>). Slightly different constants are
necessary if functional data are on an interval that is not <span
class="math inline">\([0,1]\)</span> or observations have different
spacing.</p>
</div>
<div id="plots-of-functions-used-to-generate-the-data"
class="section level3">
<h3>Plots of functions used to generate the data</h3>
<p>Below we illustrate the eigenfunctions <span
class="math inline">\(\phi_k(t)\)</span>, <span
class="math inline">\(k=1,\ldots,4\)</span> used to generate the data.
Lighter colors corresponding to larger <span
class="math inline">\(k\)</span>, the index of the function in the
basis. The first two eigenfunctions (darker shades of blue) have a
period of one (lower frequency) and the second eigenfunctions (lighter
shades of blue) have a period of two (higher frequency).</p>
<pre class="r"><code>col.me &lt;- c(&quot;darkblue&quot;, &quot;royalblue&quot;, &quot;cadetblue&quot;, &quot;deepskyblue&quot;)
plot(NULL, xlim = c(0, 1), ylim = c(-1.5, 3.5), 
     xlab = &quot;Functional domain&quot;, ylab = &quot;Functional values&quot;,
     bty = &quot;n&quot;)
for(i in 1:4){
  lines(t, phi[,i], lwd = 2.5, col = col.me[i])
}

legend(0.7, 3, legend = c(expression(phi[1](s)), expression(phi[2](s)), expression(phi[3](s)), expression(phi[4](s))),
       col = col.me, lty = c(1, 1, 1, 1), lwd = c(3, 3, 3, 3), cex = 1, bty = &quot;n&quot;)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-2-1.png" width="90%" /></p>
</div>
<div id="data-generation-and-plotting" class="section level3">
<h3>Data generation and plotting</h3>
<p>Given the data generation mechanism structure it is important to see
how data are actually obtained and what types of structures can be
generated. This highlights the fact that fPCA provides a data generating
mechanism. The general recipe is to simulate the scores independently,
multiply them with the eigenfunctions, and add noise.</p>
<pre class="r"><code>#Generate independent N(0,1) in an nxK matrix
xi &lt;- matrix(rnorm(n * K), n, K)
#Make scores on column k have variance lambda_k
xi &lt;- xi %*% diag(sqrt(lambdaTrue))
#Obtain the signal by mutliplying the scores with the eigenvectors
X &lt;- xi %*% t(phi); # of size I by J
#Add independent noise
Y &lt;- X + sigma * matrix(rnorm(n * p), n, p)</code></pre>
<p>We now visualize the results of the simulations. First we plot the
first two functions. As over plotting all functions would make the plot
unreadable, in the second plot we use a heatmap. Heatmaps can be
misleading, especially when data are noisy and the signal to noise ratio
is low.</p>
<pre class="r"><code>#Plot the first two functions simulated 
par(mar = c(4.5, 4, 0, 1))
plot(t, Y[1,], type = &quot;l&quot;, 
     col = rgb(0, 0, 1, alpha = 0.5), ylim = c(-10, 10), 
     xlab = &quot;Functional domain&quot;, ylab = &quot;Functional values&quot;, 
     bty = &quot;n&quot;)
lines(t, Y[2,], col = rgb(1, 0, 0, alpha = 0.5))</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-4-1.png" width="90%" /></p>
<pre class="r"><code>library(colorRamps)
library(viridis)
library(RColorBrewer)

colme &lt;- colorRampPalette(brewer.pal(10, &quot;RdYlBu&quot;),bias=1)(100)
#Display the heatmap of all the functions
par(mar = c(4.5, 4, 0.5, 1))
image(t(Y), xlab = &quot;Functional domain (Time)&quot;, ylab = &quot;Study participants&quot;, 
      axes = FALSE, col = colme)
mtext(text = 5 * (1:10), side = 2, line = 0.3, at = seq(1 / 10, 1, length = 10), las = 1, cex = 0.8)
mtext(text = seq(1 / 10, 1, length = 10), side = 1, line = 0.3, at = seq(1 / 10, 1, length = 10), las = 1, cex = 0.8)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-5-1.png" width="90%" /></p>
</div>
<div id="obtaining-the-fpca-results" class="section level3">
<h3>Obtaining the FPCA results</h3>
<p>Given the data generated and displayed we would like to apply
standard (raw) and functional (smooth) PCA and compare the results
obtained.</p>
<p>First conduct direct PCA analysis on the raw data without
smoothing</p>
<pre class="r"><code>results_raw &lt;- prcomp(Y)
#Obtain the estimated eigenvalues
raw_eigenvalues &lt;- results_raw$sdev ^ 2 / p

#Obtain the estimated eigenvectors
#Normalize to match with the eigenfunctions
raw_eigenvectors &lt;- sqrt(p) * results_raw$rotation

#Eigenvectors are unique up to sign
#If eigenvectors are negative correlated with the true eigenvectors
#Change their sign
for(k in 1:K){
  if(raw_eigenvectors[,k] %*% phi[,k] &lt; 0) 
    raw_eigenvectors[,k] &lt;- -raw_eigenvectors[,k]
}</code></pre>
<p>Now apply the fpca.face smoothing and display the results. The
function uses one argument, the <span class="math inline">\(n\times
p\)</span> dimensional matrix of data, where each row corresponds to one
study participant and each column corresponds to a location on the
domain (e.g., time).</p>
<pre class="r"><code>library(refund)
#Here we use more parameters for illustration purposes 
results &lt;- fpca.face(Y,center = TRUE, argvals = t, knots = 100, pve = 0.9999, var = TRUE)
#These are re-scaled to appear on the scale of the original functions 
Phi &lt;- sqrt(p) * results$efunctions
eigenvalues &lt;- results$evalues / p

#If eigenvectors are negative correlated with the true eigenvectors
#Change their sign
for(k in 1:K){
  if(Phi[,k] %*% phi[,k] &lt; 0) 
    Phi[,k] &lt;- -Phi[,k]
}</code></pre>
<p>Plot the true eigenfunctions versus estimated eigenfunctions using
raw and smooth PCA. The plot indicates that raw PCA results in highly
variable estimates of the true PCA. In contrast smooth PCA provides
smooth estimators that are more interpretable and reasonable. Smooth PCA
eigenvectors seem to track pretty closely to the mean of the
eigenvectors estimated from raw data. Both estimators do not overlap
perfectly over the true eigenvectors. This is reasonable to observe in
one simulation. However, checking whether the estimators are biased
would require multiple simulations and taking the average over these
simulations.</p>
<pre class="r"><code>par(mar = c(4.5, 4.5, 2, 2)) 
par(mfrow = c(K / 2, 2)) 
seq &lt;- (1:(p / 10)) * 10 

for(k in 1:K){
  plot(t, raw_eigenvectors[,k], type = &quot;l&quot;, col = &quot;grey70&quot;, lwd = 0.2, 
       ylim = c(-3, 3), ylab = paste(&quot;Eigenfunction &quot;, k, sep = &quot;&quot;), xlab = &quot;Time&quot;, bty = &quot;n&quot;)

  lines(t[seq], phi[seq, k], lwd = 3, col = &quot;blue&quot;, lty = 1) 
  lines(t[seq], Phi[seq, k], lwd = 3, col = &quot;red&quot;, lty = 3)
}</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-8-1.png" width="90%" /></p>
</div>
<div id="compare-the-estimated-eigenvalues-using-pca-and-fpca"
class="section level3">
<h3>Compare the estimated eigenvalues using PCA and FPCA</h3>
<p>We next compare the true eigenvalues (blue line) with the estimated
eigenvalues using raw PCA (gray dotted line) and using smooth PCA (red
dotted line). Notice that the smooth estimated eigenvalues are very
close to the true eigenvalues. In contrast, eigenvalues estimated from
raw PCA overestimate the true eigenvalues by quite a margin. This is
especially true when the true eigenvalues are zero (no signal). Raw PCA
continues to assign signal when there is none. Moreover, the estimated
eigenvalues using raw PCA decrease slowly to zero forming almost a
straight line. This feature can be observed in many applications. If
observed, it could suggest that data are noisier than what standard PCA
assumes (no noise, just smaller signals as the principal component
number increases).</p>
<pre class="r"><code>true_eigenvalues &lt;- c(lambdaTrue, rep(0, 46))
results &lt;- fpca.face(Y, center = TRUE, argvals = t, knots = 100, pve = 0.9999, var = TRUE)
smooth_eigenvalues &lt;- rep(0, 50)
ll &lt;- length(results$evalues)
smooth_eigenvalues[1:ll] &lt;- results$evalues / p

par(mar = c(4.5, 4, 0.5, 1))
plot(1:50, true_eigenvalues, lwd = 3, col = &quot;blue&quot;, bty = &quot;n&quot;, type = &quot;l&quot;,
     xlab = &quot;Index&quot;, ylab = &quot;Eigenvalues&quot;)
lines(1:50, raw_eigenvalues, lwd = 3, xlab = &quot;Index&quot;, ylab = &quot;Eigenvalues&quot;, col = &quot;grey70&quot;, lty = 3)
lines(1:50, smooth_eigenvalues, col = &quot;red&quot;, lwd = 3, lty = 3)
legend(30, 1, legend = c(&quot;True&quot;, &quot;Raw&quot;, &quot;Smooth&quot;),
       col = c(&quot;blue&quot;, &quot;grey70&quot;, &quot;red&quot;), lty = c(1, 3, 3), lwd = c(3, 3, 3), cex = 1, bty = &quot;n&quot;)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-9-1.png" width="90%" /></p>
</div>
<div id="show-how-fpca.face-works-with-nas" class="section level3">
<h3>Show how fpca.face works with NAs</h3>
<p>We now show how to use smooth functional PCA even with substantial
amount of missing observations. Start by generating a matrix of the same
dimension with Y, but containing <span
class="math inline">\(80\)</span>% missing observations</p>
<pre class="r"><code>#Missing data percentage
miss_perc &lt;- 0.8
#Randomly assign what data are missing
NA_mat &lt;- matrix(rbinom(p * n, 1, miss_perc), ncol = p)
Y_w_NA &lt;- Y
#Build the matrix with missing observations
Y_w_NA[NA_mat == 1] &lt;- NA

#Conduct fPCA on the matrix with missing data
results_NA &lt;- fpca.face(Y_w_NA, center = TRUE, argvals = t, knots = 100, pve = 0.99)

#Obtain the fPCA estimtors of eigenfunctions
ef_NA &lt;- sqrt(p) * results_NA$efunctions

#If eigenvectors are negative correlated with the true eigenvectors
#Change their sign
for(k in 1:K){
  if(ef_NA[,k] %*% phi[,k] &lt; 0) 
    ef_NA[,k] &lt;- -ef_NA[,k]
}</code></pre>
<pre class="r"><code>par(mar = c(4.5, 4.5, 2, 2)) 
par(mfrow = c(K / 2, 2)) 
seq &lt;- (1:(p / 10)) * 10 

for(k in 1:K){
  plot(t[seq], phi[seq, k], ylim = c(-3, 3), type = &quot;l&quot;, col = &quot;blue&quot;, 
       ylab = paste(&quot;Eigenfunction &quot;, k, sep = &quot;&quot;), xlab = &quot;Time&quot;, bty = &quot;n&quot;, lwd = 2)

  lines(t[seq], ef_NA[seq, k], type = &quot;l&quot;, col = &quot;red&quot;, lty = 3, lwd = 3) 
} </code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-11-1.png" width="90%" /></p>
<p>Note that even with <span class="math inline">\(80\)</span>% missing
data, the estimated eigenvectors using fPCA (red dotted line) are very
close to the true eigenvectors (blue solid lines). These estimators
start to degrade when the missing data becomes extrem and only a few
observations are available per function.</p>
</div>
</div>
<div id="application-to-nhanes" class="section level2">
<h2>Application to NHANES</h2>
<p>We begin by by reading in of the subject-level NHANES data
(nhanes_fda_with_r.rds). Note that we restrict the sample to
participants aged 5 to 80.</p>
<pre class="r"><code>## read in the data (assumes data are in the current working directory)
df_subj &lt;- read_rds(here::here(&quot;data&quot;,&quot;nhanes_fda_with_r.rds&quot;))
## filter out participants 80+ and younger than 5
df_subj &lt;-
        df_subj %&gt;% 
        filter(age &gt;= 5, age &lt; 80)</code></pre>
<div id="functional-principal-components-analysis"
class="section level3">
<h3>Functional Principal Components Analysis</h3>
<p>Next, we compare FPCA to PCA on the minute level MIMS profiles
(subject average MIMS across days). For FPCA, we apply the FACE method
(<em>refund::fpca.face</em>) using the package defaults. For PCA we
estimate the principal components using the connection between SVD and
eigen decomposition of the empirical covariance matrix.</p>
<pre class="r"><code>## Do fPCA on the subject-average MIMS profiles
MIMS_mat &lt;- unclass(df_subj$MIMS)
fpca_MIMS_subj &lt;- fpca.face(MIMS_mat)

## Do PCA on the subject-average MIMS profiles
# subtract column (minute) means to center the &quot;varianbles&quot;
MIMS_mn     &lt;- colMeans(MIMS_mat)
MIMS_mat_cn &lt;- sweep(MIMS_mat, MARGIN=2, STATS=MIMS_mn, FUN=&quot;-&quot;)
# then do SVD
svd_MIMS_subj &lt;- svd(MIMS_mat_cn)</code></pre>
<p>We can then plot the first four principal components from each
approach.</p>
<pre class="r"><code>## plot the results of PCA vs fpca
sind &lt;- seq(0,1,len=1440)
xinx &lt;- (c(1,6,12,18,23)*60+1)/1440
xinx_lab &lt;- c(&quot;01:00&quot;,&quot;06:00&quot;,&quot;12:00&quot;,&quot;18:00&quot;,&quot;23:00&quot;)
df_plt &lt;- 
        data.frame(&quot;PC&quot; = rep(paste0(&quot;PC&quot;,1:4), each=1440),
                   &quot;time&quot; = rep(sind, 4),
                   &quot;fPCA&quot; = as.vector(fpca_MIMS_subj$efunctions[,1:4]),
                   &quot;PCA&quot; = -as.vector(svd_MIMS_subj$v[,1:4]))
df_plt &lt;- 
        df_plt %&gt;% 
        pivot_longer(cols=c(&quot;fPCA&quot;,&quot;PCA&quot;))


fpca_vs_pca_NHANES &lt;- 
        df_plt %&gt;% 
        mutate(name = factor(name, levels=c(&quot;fPCA&quot;,&quot;PCA&quot;),labels=c(&quot;(A) fPCA&quot;,&quot;(B) PCA&quot;))) %&gt;% 
        ggplot() + 
        geom_line(aes(x=time, y=value, color=PC), lwd=1.25) + 
        facet_grid(~name) + 
        theme_minimal(base_size=18) + 
        xlab(&quot;Time of Day (s)&quot;) + 
        ylab(expression(&quot;Estimated Eigenfunctions (&quot; ~ phi[k](s) ~ &quot;)&quot;)) + 
        scale_x_continuous(breaks=xinx, labels=xinx_lab) + 
        theme(legend.position=c(.9,.9),
              panel.grid.major.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              axis.line.x = element_line(linewidth = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              axis.line.y = element_line(linewidth = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              strip.text = element_text(face=&quot;bold&quot;, size=18, hjust=0.03),
              axis.ticks.x = element_line(linewidth = 1),
              axis.ticks.y = element_line(linewidth = 1)) + 
        labs(color=&quot;Eigenfunction&quot;) + 
        guides(color=guide_legend(ncol=2))
fpca_vs_pca_NHANES</code></pre>
<p><img src="chapter_03_files/figure-html/plot_fpca_vs_pca-1.png" width="90%" /></p>
<p>Interpreting the functional PCs may be challenging, particularly for
PCs which explain a relatively low proportion of variance. One
visualization technique is to plot the distribution of curves which load
lowest/highest on a particular PC. Here, we plot the individuals in the
bottom and top 10% of scores for the first four PCs. The code below
calculates these quantities.</p>
<pre class="r"><code>## plot individual curves loading highly/lowly on the first 4 PCs
# set the seed for reproducibility
set.seed(1983)
# number of eigenfunctions to plot
K &lt;- 4
# number of sample curves to plot for each PC
n_plt &lt;-  10
# create container data frame for storing/plotting the individual curves
# for high/low loadings on each of the K PCs
# this data frame will have 
#  2 * K * n_plt * 1440 rows
#  2 = high/low
#  K = # of PCs
#  n_plt = number of individual curves to plot
#  1440 = minutes in a day
df_plt_ind &lt;- expand.grid(&quot;sind&quot; = sind,
                          &quot;id&quot; = 1:n_plt,
                          &quot;high&quot; = c(&quot;high&quot;,&quot;low&quot;),
                          &quot;PC&quot; = paste0(&quot;PC &quot;,1:4)) %&gt;% 
        mutate(high = relevel(high, ref=&quot;low&quot;))
# create container data frame for storing/plotting the average curves
# for high/ow loadings on each of the first K PCs
# this data frame will have 
#  2 * K * 1440 rows
#  2 = high/low
#  K = # of PCs
#  1440 = minutes in a day
df_plt_ind_mu &lt;- expand.grid(&quot;sind&quot; = sind,
                             &quot;high&quot; = c(&quot;high&quot;,&quot;low&quot;),
                             &quot;PC&quot; = paste0(&quot;PC &quot;,1:4),
                             &quot;value&quot; = NA)%&gt;% 
        mutate(high = relevel(high, ref=&quot;low&quot;))
# NOTE: the code below assumes an ordering of rows implied by the construction
#       used by the expand.grid function

# create empty vectors to store the average and individual curves, respectively
mu_vec &lt;- c()
ind_vec &lt;- c()
# loop over eigenfunctions
for(k in 1:K){
        # get the 10th and 90th percentiles of the scores
        q_k &lt;- quantile(fpca_MIMS_subj$scores[,k],c(0.1,0.9))
        # get indices associated with low/high scores
        inx_low_k &lt;- which(fpca_MIMS_subj$scores[,k] &lt;= q_k[1])
        inx_high_k &lt;- which(fpca_MIMS_subj$scores[,k] &gt; q_k[2])
        # get the average curves in each quantile: 0-.1, .9-1
        mu_low_k &lt;- colMeans(fpca_MIMS_subj$Yhat[inx_low_k,])
        mu_high_k &lt;- colMeans(fpca_MIMS_subj$Yhat[inx_high_k,])
        # concatenate with existing mean vector
        mu_vec &lt;- c(mu_vec, mu_low_k, mu_high_k)
        # get individual curves for the n_plt randomly selected participants
        val_low_k &lt;- as.vector(t(fpca_MIMS_subj$Yhat[sample(inx_low_k, size=n_plt),]))
        val_high_k &lt;- as.vector(t(fpca_MIMS_subj$Yhat[sample(inx_high_k, size=n_plt),]))
        # concatenate with existing individual vector
        ind_vec &lt;- c(ind_vec, val_low_k, val_high_k)
}
# add the mean and individual vectors into the data frames constructed before the loop
df_plt_ind_mu$value &lt;- mu_vec
df_plt_ind$value &lt;- ind_vec</code></pre>
<p>We can then plot the average and individual curves. We do see the
individual curves which load highly on each of the first four PCs do, on
average, largely reflect the shapes of the PCs, with this visual effect
most strong for the first three PCs.</p>
<pre class="r"><code>plt_ind &lt;- 
        df_plt_ind %&gt;% 
        mutate(id = factor(id),
               id_high = paste0(id,&quot;_&quot;,high)) %&gt;%
        ggplot() + 
        geom_line(aes(x=sind, y=value, group=id_high, color=high),alpha=0.5) + 
        facet_wrap(~PC,ncol=2) + 
        theme_minimal(base_size=18) + 
        scale_x_continuous(breaks=xinx, labels=xinx_lab) + 
        theme(legend.position=&quot;none&quot;,
              panel.grid.major.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              axis.line.x = element_line(size = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              axis.line.y = element_line(size = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              strip.text = element_text(face=&quot;bold&quot;, size=18, hjust=0.03),
              axis.ticks.x = element_line(size = 1),
              axis.ticks.y = element_line(size = 1)) + 
        xlab(&quot;Time of Day (s)&quot;) + ylab(expression(&quot;MIMS: &quot; ~ W[i](s))) + 
        geom_line(aes(x=sind, y=value, group=high, color=high),
                  data=df_plt_ind_mu, lwd=2) 
## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.
## ℹ Please use the `linewidth` argument instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
plt_ind</code></pre>
<p><img src="chapter_03_files/figure-html/plt_high_low-1.png" width="90%" /></p>
</div>
</div>
<div id="generalized-functional-principal-components-analysis-gfpca"
class="section level2">
<h2>Generalized Functional Principal Components Analysis (GFPCA)</h2>
<p>Functional data come in many forms. For many types of continuous
functional data, the Gaussian assumption may not be reasonable. In this
case, it may be be possible to apply a set of transformations <span
class="math inline">\(g_s:\mathbb{R}\rightarrow\mathbb{R}\)</span> such
that <span class="math inline">\(g_s\{W_i(s)\}\)</span> is approximately
Gaussian for every <span class="math inline">\(s\in S\)</span>. In this
case PCA and FPCA can be applied to the transformed data. However are
not be always available or desirable. Moreover, when data are not
continuous (e.g., binary) transformations do not solve the underlying
problem. However, we can still apply the principals of FPCA if we assume
a the data are generated from a latent Gaussian process.</p>
<p>Suppose <span class="math inline">\(W_i(s)\)</span>, <span
class="math inline">\(s\in S\)</span> are non-Gaussian observed data. We
assume that <span class="math inline">\(W_i(s_j)\)</span> have a
distribution with mean <span
class="math inline">\(E\{W_i(s_j)\}=\mu_i(s_j)\)</span>, where <span
class="math display">\[\begin{equation*}
    \eta_i(s_j)=h\{\mu_i(s_j)\}=X_i(s_j)=\beta_0(s_j) +
\sum_{k=1}^K\xi_{ik}\phi_k(s_j)\;,
\end{equation*}\]</span> <span
class="math inline">\(\eta_i(\cdot)\)</span> is the linear predictor,
<span class="math inline">\(h(\cdot)\)</span> is a link function, <span
class="math inline">\(\beta_0(\cdot)\)</span> is a function that varies
smoothly along the domain <span class="math inline">\(S\)</span>, <span
class="math inline">\(\phi_k(\cdot)\)</span> are assumed to be
orthonormal functions in the <span class="math inline">\(L_2\)</span>
norm, and <span class="math inline">\(\xi_{ik}\)</span> are mutually
uncorrelated random variables. The distribution of <span
class="math inline">\(W_i(s_j)\)</span> could belong to the exponential
family or any type of non-Gaussian distribution. The expectation is that
the number of orthonormal functions, <span
class="math inline">\(K\)</span>, is not too large and the functions
<span class="math inline">\(\phi_k(\cdot)\)</span>, <span
class="math inline">\(k=1,\ldots,K\)</span>, are unknown and will be
estimated from the data. The advantage of this conceptual model is that,
once the functions <span class="math inline">\(\phi_k(\cdot)\)</span>
are estimated, the model becomes a generalized linear mixed model (GLMM)
with a small number of uncorrelated random effects. This transforms the
high-dimensional functional model with non-Gaussian measurements into a
low-dimensional GLMM that can be used for estimation and inference.</p>
<div id="application-to-nhanes-1" class="section level3">
<h3>Application to NHANES</h3>
<p>The fast GFPCA method is first shown as presented in the book, as a
4-step procedure estimated manually using mase <strong>R</strong>
functions in combination with <strong>mgcv</strong>. Afterward, we show
how to fit the same GFPCA model using the <strong>fastGFPCA</strong>
package, which wraps this code into a single estimation function.</p>
<p>The GFPCA method decribed in the book is applied to binarized
physical activity (active/inactive) profiles. There are many possible
ways to define such a profile. For the purposes of this application, we
chose to first binarize the day-level data, then take the median across
days (rounding up in the event of a tie). To do this we first read in
the multi-level (day-level) physical activity data.</p>
<pre class="r"><code>df_gfpca_subj_day &lt;- read_rds(here::here(&quot;data&quot;,&quot;nhanes_fda_with_r_ml.rds&quot;))</code></pre>
<p>The next step is to binarize the day level data, then take the median
across days. Here we use a threshold of 10.558 which was suggested by a
recently published paper comparing MIMS to activity counts.</p>
<pre class="r"><code>## Y_ij(s)
# extract the day-level activity data
Ymat &lt;- unclass(df_gfpca_subj_day$MIMS)
## Y_ij^B(s)
# binarize the data (this vectorizes the matrix), then put back in matrix format
YBmat &lt;- matrix(as.numeric(Ymat &gt;= 10.558), nrow(Ymat), ncol(Ymat), byrow=FALSE)

## Create X_i(s)
# get unique IDs for all participants and their corresponding row indices
uid &lt;- unique(df_gfpca_subj_day$SEQN)
nid &lt;- length(uid)
inx_rows &lt;- split(1:nrow(YBmat), factor(df_gfpca_subj_day$SEQN, levels=uid))
# calculate X_i(s)
Xmat &lt;- t(vapply(inx_rows, function(x) round(colMeans(YBmat[x,,drop=FALSE])),
                 numeric(1440)))
rm(Ymat, YBmat)</code></pre>
<p>Having created our binarized activity profiles, we then set up a long
data matrix which will be used for estimation.</p>
<pre class="r"><code>## Put the data in long format
sind &lt;- 1:1440
df_gfpca &lt;- data.frame(SEQN = as.factor(rep(uid, each=1440)),
                       X = as.vector(t(Xmat)),
                       sind = sind)</code></pre>
<p>To show what these binarized activity profiles look like, we plot
profiles for four randomly selected participants.</p>
<pre class="r"><code>## plot 4 profiles
set.seed(191)
n_plt &lt;- 4
ids_plt &lt;- sample(uid, size=n_plt, replace=F)
plt_profiles_gfpca &lt;- 
        df_gfpca %&gt;% 
        # filter(SEQN %in% paste0(&quot;SEQN &quot;, ids_plt)) %&gt;% 
        filter(SEQN %in% as.character(ids_plt)) %&gt;%
        mutate(sind_bin = floor(sind/60)) %&gt;% 
        group_by(SEQN, sind_bin) %&gt;% 
        mutate(sind_bin = mean(sind),
                  X_bin = mean(X)) %&gt;% 
        ungroup() %&gt;% 
        ggplot() + 
        geom_point(aes(x=sind, y=X), alpha=0.25) + 
        geom_line(aes(x=sind_bin, y=X_bin),linewidth=1.5,color=&quot;red&quot;) + 
        facet_wrap(~SEQN, ncol=2) + 
        theme_minimal(base_size=18) + 
        scale_x_continuous(breaks=xinx*1440, labels=xinx_lab) + 
        scale_y_continuous(breaks=c(0,1), labels=c(&quot;0 (Inactive)&quot;,&quot;1 (Active)&quot;)) + 
        theme(legend.position=&quot;none&quot;,
              panel.grid.major.y = element_blank(),
              panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank(),
              panel.grid.minor.y = element_blank(),
              axis.line.x = element_line(size = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              axis.line.y = element_line(size = 1, linetype = &quot;solid&quot;, colour = &quot;black&quot;),
              strip.text = element_text(face=&quot;bold&quot;, size=18, hjust=0.03),
              axis.ticks.x = element_line(size = 1),
              axis.ticks.y = element_line(size = 1)) + 
        xlab(&quot;Time of Day (s)&quot;) + ylab(&quot;&quot;)  </code></pre>
<p>Now, although the fast GFPCA methodology described in the book can
indeed handle data on the order of the NHANES data, the execution time
is still relatively long when compared to the expected compile times for
a markdown document. To facilite fast compilation, here we restrict the
sample to a subset of <span class="math inline">\(N=300\)</span>
participants. Users interested in obtaining the full results need only
comment out the chunk below.</p>
<pre class="r"><code>set.seed(5)
N_sub &lt;- 300
uid_sub &lt;- sample(uid, size=N_sub, replace=F)
df_gfpca &lt;- df_gfpca %&gt;% filter(SEQN %in% uid_sub)</code></pre>
<p>The fast GFPCA method starts by fixing a value <span
class="math inline">\(s\in S\)</span> and a neighborhood <span
class="math inline">\(N_s\)</span> around <span
class="math inline">\(s\)</span>. The following random intercept model
is then estimated <span class="math display">\[\begin{equation}
    \eta_i(s_j)=h\{\mu_i(s_j)\}=\beta_0(s)+X_i(s)\;\;{\rm
for}\;\;{s_j\in N_s},
    \label{eq:ch3_n4}
\end{equation}\]</span> where <span
class="math inline">\(\beta_0(s)\)</span> is the fixed intercept and
<span class="math inline">\(X_i(s)\sim N(0,\sigma^2_s)\)</span> are
independent random intercepts for <span class="math inline">\(s\in
N_s\)</span> and <span class="math inline">\(i=1,\ldots,n\)</span>, and
<span class="math inline">\(\eta_i(s)=\beta_0(s)+X_i(s)\)</span> is the
linear predictor. We refer to the set of neighborhoods as bins, and
define each bin by it’s midpoint and the neighborhood. We refer readers
to the text for additional details. This choice is non-trivial, and open
questions remain regarding how to optimally choose these bins. At a high
level, these bins should cover the entire functional domain, and each
bin should have enough data so that the local models are estimable
(i.e., the data are not all 0 or all 1 in the case of binary data), and
not be so wide as to cover regions where the underlying functions are
expected to exhibit a high degree of curvature.</p>
<p>This approach will produce estimators of <span
class="math inline">\(X_i(s)\)</span> at every value of <span
class="math inline">\(s\in S\)</span>. FPCA can then be applied to
smooth these estimators across <span class="math inline">\(s\)</span>.
An advantage of this approach is that it can be used with any
distribution of <span class="math inline">\(W_i^*(s_j)\)</span> as long
as the single-level mixed effects model can be fit and is readily
extended to such scenarios as covariate adjusted models and
multilevel/structured functional data (manuscripts in progress).</p>
<p>The code below shows how to partition the 24 hour domain in 20 minute
wide bins and fit the local GLMMs. For simplicity of presentation, here
we do not take advantage of they cyclical nature of the data by creating
bins which cross midnight (see the fastGFPCA function at the end of this
section). Note that because we have subset the data (N=300) and there
are a high proportion of 0s during the nighttime hours, you will see
some convergence warnings. In practice these should be carefully
inspected and evaluate whether bins need to be modified (e.g., widened
during the nighttime).</p>
<p>Note that to be consistent with the behavior of the
<strong>fast_gfpca</strong> wrapper function, we use fast method for
estimation of the local GLMMs (nAGQ=0 argument to the
<strong>glmer</strong> function).</p>
<pre class="r"><code>## Step 1: choose bins
# bin width (minutes)
w &lt;- 20
#Add column for bin indicator
df_gfpca_local &lt;- 
        df_gfpca %&gt;%
        mutate(sind_w = floor(sind / w) * w + w / 2)
## Step 2: Fit separate GLMMs for each bin
fit_gfpca_local &lt;-
        df_gfpca_local %&gt;%
        nest_by(sind_w) %&gt;%
        mutate(fit = list(glmer(X ~ 1 + (1|SEQN),
                                data = data, family = binomial,
                                nAGQ = 0))) %&gt;%
        summarize(y_i = coef(fit)$SEQN[[1]]) %&gt;%
        ungroup()
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in
## dplyr 1.1.0.
## ℹ Please use `reframe()` instead.
## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`
##   always returns an ungrouped data frame and adjust accordingly.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
## `summarise()` has grouped output by &#39;sind_w&#39;. You can override using the
## `.groups` argument.</code></pre>
<p>Next, for Step 3, we extract the estimated linear predictor for each
paritcipant and perform FPCA on those predicted values via the
<strong>fpca.face</strong> function.</p>
<pre class="r"><code>#Re-arrange predicted log odds to wide format
etamat &lt;- matrix(fit_gfpca_local$y_i, N_sub, 1440 / w + 1)
#Estimate FPCA
fPCA_local &lt;- fpca.face(etamat, pve = 0.95)</code></pre>
<p>If we plot the first four eigenfunctions (see below), we find that
the shapes are extremely similar to those presented in the text.</p>
<pre class="r"><code>sind_bin &lt;- sort(unique(fit_gfpca_local$sind_w))
matplot(sind_bin, fPCA_local$efunctions[,1:4],type=&#39;l&#39;, xlab=&quot;Time of Day (s)&quot;, ylab=expression(hat(phi)[k](s)), xaxt=&#39;n&#39;)
axis(1, at=c(1,6,12,18,23)*60+1, labels=c(&quot;01:00&quot;,&quot;06:00&quot;,&quot;12:00&quot;,&quot;18:00&quot;,&quot;23:00&quot;))</code></pre>
<p><img src="chapter_03_files/figure-html/fastGPFCA_step3_plot-1.png" width="90%" /></p>
<p>We then need to evaluate the estimated eigenfunctions on the original
grid. The code below shows how to do this using linear interpolation (as
presented in the text). These re-evaluated eigenfunctions are added to
the data frame used for model fitting to include as random slopes in our
final model. Here we choose to use 4 eigenfunctions.</p>
<p>Note that in the <strong>fastGFPCA</strong> function we evaluate the
eigenfunctions exactly using the fact that the eigenfunctions provided
by the <strong>fpca.face</strong> algorithm are a linear combination of
B-splines.</p>
<pre class="r"><code>K &lt;- 4
#Linear interpolation of eigenfunctions on original grid
sind_bin &lt;- sort(unique(df_gfpca_local$sind_w))
phi_interp &lt;- matrix(NA, 1440, 4)
colnames(phi_interp) &lt;- paste0(&quot;phi_&quot;, 1:K)
for(k in 1:K) {
        phi_local &lt;- fPCA_local$efunctions[,k]
        interp_k &lt;- approx(sind_bin, y=phi_local, xout=sind)
        phi_interp[,k] &lt;- interp_k$y
}
phi_interp &lt;- data.frame(phi_interp)
phi_interp$sind &lt;- sind
#Add these interpolated eigenfunctions to the long format dataframe
df_gfpca &lt;- left_join(df_gfpca, phi_interp, by = &quot;sind&quot;)
uid &lt;- unique(df_gfpca$SEQN)
set.seed(19111)
uid_sub &lt;- c(paste0(&quot;SEQN SEQN &quot;, c(ids_plt)), sample(uid, size=300, replace=F))
df_gfpca_sub &lt;- 
        df_gfpca %&gt;% 
        filter(SEQN %in% uid_sub) %&gt;% 
        mutate(SEQN = factor(str_extract(SEQN, &quot;[0-9]+&quot;))) %&gt;% 
        drop_na()</code></pre>
<p>Finally, for Step 4, we re-estimate the model conditional on the
estimated eigenfunctions.</p>
<pre class="r"><code>#Estimate the GLMM
fit_gfpca &lt;-
        bam(X ~ s(sind, bs = &quot;cc&quot;, k = 20) +
                    s(SEQN, by = phi_1, bs = &quot;re&quot;) +
                    s(SEQN, by = phi_2, bs = &quot;re&quot;) +
                    s(SEQN, by = phi_3, bs = &quot;re&quot;) +
                    s(SEQN, by = phi_4, bs = &quot;re&quot;),
            method = &quot;fREML&quot;, discrete = TRUE, data = df_gfpca, 
            family=binomial)</code></pre>
<p>Alternatively, the method can be estimated using the convenient
wrapper function <strong>fast_gfpca</strong> in the cleverly named
<strong>fastGFPCA</strong> package. Note that the data input for this
wrapper function requires a particular naming convention. Specifically,
“id” for the participant identifier, “value” for the functional data,
and “index” for the functional domain. The <strong>fast_gfpca</strong>
supports cyclic/periodic data using the “periodicity” argument. Below,
we estimate fast GFPCA on the NHANES data using non-overlapping bins of
20 minutes and 4 eigenfunctions for the latent Gaussian process.</p>
<pre class="r"><code>library(fastGFPCA)
df_gfpca &lt;- 
  df_gfpca %&gt;% 
  mutate(id = SEQN, value = X, index = sind)
fit_gfpca_wrap &lt;- fast_gfpca(Y=df_gfpca, overlap = F, binwidth=20, family=&quot;binomial&quot;,
                             periodicity = TRUE, npc=4)</code></pre>
<p>We can inspect the contents of the returned object. The function
returns the final estimates for the linear predictor (Yhat), the
estimated subject-specific scores from step 4 (scores), the population
mean function (mu), the estiamted eigenfunctions (efunctions), and the
estimated eigenvalues from Step 3 (evalues).</p>
<pre class="r"><code>str(fit_gfpca_wrap)
## List of 8
##  $ Yhat      : num [1:300, 1:1440] -5.35 -4.28 -4.73 -4.63 -2.99 ...
##  $ scores    : num [1:300, 1:4] 1.341 0.489 -0.391 1.1 -0.658 ...
##  $ mu        : Named num [1:1440] -3.74 -3.76 -3.76 -3.77 -3.78 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:1440] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ efunctions: num [1:1440, 1:4] -0.0706 -0.0721 -0.0736 -0.075 -0.0765 ...
##  $ evalues   : num [1:4] 126.7 59 20.5 12.2
##  $ npc       : num 4
##  $ pve       : num NA
##  $ family    :List of 12
##   ..$ family    : chr &quot;binomial&quot;
##   ..$ link      : chr &quot;logit&quot;
##   ..$ linkfun   :function (mu)  
##   ..$ linkinv   :function (eta)  
##   ..$ variance  :function (mu)  
##   ..$ dev.resids:function (y, mu, wt)  
##   ..$ aic       :function (y, n, mu, wt, dev)  
##   ..$ mu.eta    :function (eta)  
##   ..$ initialize: language {     if (NCOL(y) == 1) { ...
##   ..$ validmu   :function (mu)  
##   ..$ valideta  :function (eta)  
##   ..$ simulate  :function (object, nsim)  
##   ..- attr(*, &quot;class&quot;)= chr &quot;family&quot;
##  - attr(*, &quot;class&quot;)= chr &quot;fpca&quot;</code></pre>
<p>Below we plot the estimated eigenfunctions obtained from the
<strong>fastGFPCA::fast_gfpca</strong> function and from our manual
coding of the fast GFPCA methodology for comparison. We see the
estimates are effectively identical. Note that 1) the sign of the first
and fourth eigenfunction was changed for ease of visual comparison
(recall that eigenfunctions/scores are unique up to a sign); and 2) the
magnitude of the eigenfunctions are not the same. This is because the
<strong>fpca.face</strong> function returns orthonormal vectors (does
not account for the scale of the domain). The
<strong>fastGFPCA::fast_gfpca</strong> rescales the vectors internally
so that the <em>eigenfunctions</em> are orthonormal.</p>
<pre class="r"><code>par(mfrow=c(1,2))
matplot(sind_bin, fPCA_local$efunctions[,1:4]*kronecker(t(c(-1,1,-1,1)), rep(1,nrow(fPCA_local$efunctions))),
        type=&#39;l&#39;, xlab=&quot;Time of Day (s)&quot;,
        main=&quot;&quot;, ylab=expression(hat(phi)[k](s)), xaxt=&#39;n&#39;)
axis(1, at=c(1,6,12,18,23)*60+1, labels=c(&quot;01:00&quot;,&quot;06:00&quot;,&quot;12:00&quot;,&quot;18:00&quot;,&quot;23:00&quot;))

matplot(1:1440, fit_gfpca_wrap$efunctions[,1:4],type=&#39;l&#39;, xlab=&quot;Time of Day (s)&quot;,
        main=&quot;fastGFPCA::fast_gfpca&quot;,ylab=expression(hat(phi)[k](s)), xaxt=&#39;n&#39;)
axis(1, at=c(1,6,12,18,23)*60+1, labels=c(&quot;01:00&quot;,&quot;06:00&quot;,&quot;12:00&quot;,&quot;18:00&quot;,&quot;23:00&quot;))</code></pre>
<p><img src="chapter_03_files/figure-html/fast_gfpca_package_return_plot-1.png" width="90%" /></p>
</div>
</div>
<div id="sparse-fpca-of-the-content-data-set" class="section level2">
<h2>Sparse FPCA of the CONTENT data set</h2>
<p>Load the packages that will be used. Here the function
<code>face::face.sparse</code> is doing the heavy lifting. The package
<code>tidyverse</code> is used for reading the data and some data
manipulation, the package <code>fields</code> is used for its
<code>image.plot</code> function, while <code>refund</code> contains the
CONTENT data set.</p>
<pre class="r"><code>library(tidyverse)
library(refund)
library(face)
library(fields)</code></pre>
<div id="obtain-and-describe-the-data" class="section level3">
<h3>Obtain and describe the data</h3>
<p>Data are available in the <code>refund</code> package</p>
<pre class="r"><code>data(&quot;content&quot;)
content_df &lt;- content
head(content_df)
##   id ma1fe0 weightkg height agedays     cbmi  zlen zwei zwfl zbmi
## 1  1      0    5.618   56.0      61 17.91454 -0.53 0.70 1.64 1.37
## 2  1      0    5.990   56.2      65 18.96506 -0.62 1.05 2.18 1.93
## 3  1      0    5.974   56.5      71 18.71407 -0.75 0.81 1.99 1.69
## 4  1      0    6.290   57.4      77 19.09092 -0.57 1.01 2.03 1.83
## 5  1      0    6.618   58.6      84 19.27221 -0.28 1.20 1.94 1.85
## 6  1      0    6.530   58.8      91 18.88681 -0.46 0.89 1.70 1.56</code></pre>
<pre class="r"><code>nobs &lt;- dim(content_df)[1]
id &lt;- content_df$id
t &lt;- content_df$agedays
uid &lt;- unique(id)
nbabies &lt;- length(uid)
av_obs &lt;- round(nobs / nbabies, digits = 0)</code></pre>
<p>We now illustrate the sampling locations of the CONTENT data, where
each study participant is shown on one line. Each dot in the plot below
is a sampling point for a child expressed in days from birth.</p>
<pre class="r"><code>par(mar = c(4.5, 4, 0.5, 1))
plot(1, type = &quot;n&quot;, xlab = &quot;Days from birth&quot;, ylab = &quot;Study participants&quot;, xlim = c(0, 700), ylim = c(0, length(uid)), bty = &quot;n&quot;) 

for(i in 1:length(uid)){
  seq &lt;- which(id == uid[i])
  points(t[seq], rep(i, length(seq)), col = &quot;red&quot;, pch = 19, cex = 0.2)
}</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-15-1.png" width="90%" /></p>
<p>We now display the z-score for length (blue dots) and weight (red
dots) for <span class="math inline">\(4\)</span> study participants.
This plot illustrates the dependence between the normalized length and
weight measures as a function of time from birth.</p>
<pre class="r"><code>y1 &lt;- content_df$zlen
y2 &lt;- content_df$zwei
Sample &lt;- c(30, 45, 56, 67)

par(mfrow = c(2, 2), mar = c(4.5, 4.5, 3, 2))
for(i in 1:4){
  sel &lt;- which(id == uid[Sample[i]])
  plot(t[sel], y1[sel], col = &quot;blue&quot;, pch = 19, 
       xlab = &quot;Days from birth&quot;,
       ylab = &quot;z-score&quot;, main = paste(&quot;Subject &quot;, uid[Sample[i]]), bty = &quot;n&quot;, 
       ylim = c(-4, 4), xlim = c(0, 701), cex = 0.7)
  points(t[sel], y2[sel], col = &quot;red&quot;, pch = 19, cex = 0.7)
}</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-16-1.png" width="90%" /></p>
</div>
<div id="estimation-using-face.sparse" class="section level3">
<h3>Estimation using face.sparse</h3>
<p>This component of the document illustrates how to use the
<code>face.sparse</code> function to fit a sparse FPCA model and extract
important summaries of the fit. Data needs to be in long format with y
containing the data, id containing the subject ids, and t containing
time. We build a data frame with these values.</p>
<pre class="r"><code>id &lt;- content_df$id
uid &lt;- unique(id)
t &lt;- content_df$agedays
y &lt;- content_df$zlen

#Prepare the data for face.sparse
data &lt;- data.frame(y = y, argvals = t, subj = id)

#Apply face.sparse to the entire data set
fit_face &lt;- face.sparse(data, argvals.new = (1:701))

#Create a data double
data.h &lt;- data
tnew &lt;- fit_face$argvals.new</code></pre>
<p>All results are now stored in the <code>fit_face</code> variable and
we will show how to extract and plot estimators of the mean, covariance,
variance, correlation function, and eigenfunctions.</p>
<pre class="r"><code>#Smooth marginal mean
m &lt;- fit_face$mu.new
#Smooth covariance
Cov &lt;- fit_face$Chat.new
#Pointwise covariance
Cov_diag &lt;- diag(Cov)
#Smooth correlation 
Cor &lt;- fit_face$Cor.new
#Pointwise prediction intervals
m_p_2sd &lt;- m + 2 * sqrt(Cov_diag)
m_m_2sd &lt;- m - 2 * sqrt(Cov_diag)
#Smooth eigenfunctions 
eigenf &lt;- fit_face$eigenfunctions
#Smooth eigenvalues 
eigenv &lt;- fit_face$eigenvalues</code></pre>
<p>Plot the mean, variance, marginal prediction interval, and
correlation function. Note that the mean function (panel in first row
first column) has an increasing trend with a decline between <span
class="math inline">\(200\)</span> and <span
class="math inline">\(400\)</span> days after birth. This is likely to
be due to what babies remained in the study after day <span
class="math inline">\(200\)</span>. It could be that lighter babies were
more likely to stay in the study, but this hypothesis requires more
investigation. The standard deviation (panel in first row second column)
is very close to <span class="math inline">\(1\)</span>, but smaller
than <span class="math inline">\(1\)</span>. This is probably due to the
nature of the sampling, which focuses on a particular subgroup of
babies. The pointwise prediction interval (panel in second row first
column) seems to indicate that the assumption of normality of the
marginal distributions at every time point may be reasonable. The
correlation plot (panel in second row second column) seems to indicate
that correlation is not overly complicated, and indicates that most of
the variability in the smooth component may be explained by a few
eigenfunctions.</p>
<pre class="r"><code>par(mfrow = c(2, 2), mar = c(4.5, 4.5, 1, 2))
#First panel, plot the mean function
plot(tnew, m, 
     lwd = 2, lty = 2, col = &quot;blue&quot;, 
     xlab = &quot;Days from birth&quot;, 
     ylab = &quot;Mean zlen&quot;, bty = &quot;n&quot;)

#Second panel, plot the standard deviation
plot(tnew, sqrt(Cov_diag), type = &quot;l&quot;, lwd = 3, 
     col = &quot;red&quot;,
     xlab = &quot;Days from birth&quot;,
     ylab = &quot;SD zlen&quot;, bty = &quot;n&quot;)

#Third panel, plot the mean plus minus 2sd
plot(1, type = &quot;n&quot;, xlab = &quot;Days from birth&quot;, ylab = &quot;zlen&quot;, xlim = c(0, 700), 
     ylim = c(-3.5, 3), bty = &quot;n&quot;) 

for(i in 1:length(uid)){ 
  seq &lt;- which(id == uid[i])
  lines(t[seq], y[seq], col = rgb(0, 0, 0, alpha = 0.1)) 
}

#Mean and marginal prediction intervals
lines(tnew, m, lwd = 3, lty = 1, col = &quot;blue&quot;)
lines(tnew, m_p_2sd, lwd = 2, lty = 3, col = &quot;red&quot;)
lines(tnew, m_m_2sd, lwd = 2, lty = 3, col = &quot;red&quot;)

#Fourth panel, correlation function
image.plot(tnew, tnew, Cor, 
           xlab = &quot;Days from birth&quot;, 
           ylab = &quot;Days from birth&quot;,
           main = &quot;&quot;,
           axis.args = list(at = c(0.6, 0.7, 0.8, 0.9, 1.0)),
           legend.shrink = 0.8, 
           legend.line = -1.5, legend.width = 0.5)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-19-1.png" width="90%" /></p>
<p>The first three eigenvalues of the covariance operator explain more
than <span class="math inline">\(99\)</span>% of the variability of the
smooth component. Below we display the eigenfunctions that correspond to
these three largest eigenvalues. The first eigenfunction (PC1, shown in
blue) is very close to a random intercept, while the second
eigenfunction (PC2, shown in green) is very close to a random slope.
However, the third eigenfunction (PC3, shown in red) could probably be
captured by a quadratic spline with a knot at <span
class="math inline">\(300\)</span> days from birth.</p>
<pre class="r"><code>par(mar = c(4.5, 4.5, 0, 2))
col_me &lt;- c(&quot;cornflowerblue&quot;, &quot;aquamarine3&quot;, &quot;coral1&quot;)

#Make an empty plot
plot(1, type = &quot;n&quot;, xlab = &quot;Days from birth&quot;, ylab = &quot;&quot;, xlim = c(0, 700), 
     ylim = c(-3.5, 5), bty = &quot;n&quot;)

for(i in 1:3){
  lines(0:700, eigenf[,i], col = col_me[i], lwd = 3)
}
legend(500, 4, c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;), 
       col = col_me, lty = c(1, 1, 1),
       bty = &quot;n&quot;, lwd = c(3, 3, 3), cex = 0.8)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-20-1.png" width="90%" /></p>
</div>
<div id="predict-trajectories-using-face.sparse" class="section level3">
<h3>Predict trajectories using face.sparse</h3>
<p>Predict and plot the z-score for length observations for four study
participants. These are the same study participants shown in the data
examples earlier.</p>
<pre class="r"><code>Sample &lt;- c(30, 45, 56, 67)

#The days where we predict the function
seq &lt;- 1:701
#This is used to prepare the data used for prediction
k &lt;- length(seq)
par(mfrow = c(2, 2), mar = c(4.5, 4.5, 3, 2))
for(i in 1:4){
  #Begin for loop over study participants
  #Select the data that correspond to individual i in Sample
  sel &lt;- which(id == uid[Sample[i]])
  dati &lt;- data.h[sel,]
  
  #Create the data frame for prediction
  dati_pred &lt;- data.frame(y = rep(NA, nrow(dati) + k),
                          argvals = c(rep(NA, nrow(dati)), seq),
                          subj = rep(dati$subj[1], nrow(dati) + k ))
  
  #Fill the first part of the data set with the observations for the subject that will be predicted
  dati_pred[1:nrow(dati),] &lt;- dati
  #Produce the predictions for subject i
  yhat2 &lt;- predict(fit_face, dati_pred)
  
  data3 &lt;- dati
  Ylim &lt;- range(c(data3$y, yhat2$y.pred))
  Ord &lt;- nrow(dati) + 1:k
  
  plot(data3$argvals, data3$y,
       xlab = &quot;Days from birth&quot;,
       ylab = &quot;zlen&quot;, pch = 19, col = &quot;blue&quot;,
       cex = 0.6, ylim = c(-3, 3),
       cex.lab = 1.25, cex.axis = 1.25,
       cex.main = 1.25, xlim = c(1, 700), 
       bty = &quot;n&quot;, main = paste(&quot;Subject &quot;, uid[Sample[i]]))
  lines(dati_pred$argvals[Ord], yhat2$y.pred[Ord], col = &quot;red&quot;, lwd = 2)
  lines(dati_pred$argvals[Ord], yhat2$y.pred[Ord] - 1.96 * yhat2$se.pred[Ord], 
        col = &quot;red&quot;, lwd = 1, lty = 2)
  lines(dati_pred$argvals[Ord], yhat2$y.pred[Ord] + 1.96 * yhat2$se.pred[Ord], 
        col = &quot;red&quot;, lwd = 1, lty = 2)
  lines(tnew, fit_face$mu.new, col = &quot;black&quot;, lwd = 2)
}#End for loop over study participants</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-21-1.png" width="90%" /></p>
</div>
</div>
<div id="when-pca-fails" class="section level2">
<h2>When PCA Fails</h2>
<div id="pca-for-pure-noise" class="section level3">
<h3>PCA for pure noise</h3>
<p>We generate data independently from <span
class="math display">\[W_i(t)\sim N(0,\sigma^2)\]</span> and apply PCA
to see what types of results we obtain.</p>
<p>The number of study participants is set to <span
class="math inline">\(n=250\)</span> and the number grid points is set
to <span class="math inline">\(p=3000\)</span> to illustrate a case of
high dimensional data.</p>
<pre class="r"><code>set.seed(5242022)
n &lt;- 250
p &lt;- 3000
K &lt;- 4
t &lt;- (1:p) / p
sigma &lt;- 2
W &lt;- sigma * matrix(rnorm(n * p), n, p)</code></pre>
<p>Apply PCA to the data matrix <span
class="math inline">\(W\)</span></p>
<pre class="r"><code>results_raw &lt;- prcomp(W)
#Obtain the estimated eigenvalues
raw_eigenvalues &lt;- results_raw$sdev ^ 2 / 3000

#Obtain the estimated eigenvectors
#Normalize to match with the eigenfunctions
raw_eigenvectors &lt;- sqrt(p) * results_raw$rotation</code></pre>
<p>Plot the eigenvectors and save the figure</p>
<pre class="r"><code>par(mar = c(4.5, 4.5, 2, 2))
par(mfrow = c(K / 2, 2))
seq &lt;- (1:(p / 10)) * 10
for(k in 1:K){
  plot(t, raw_eigenvectors[,k], type = &quot;l&quot;, col = &quot;grey70&quot;, lwd = 0.2, ylim = c(-3, 3), 
       ylab = paste(&quot;Eigenfunction &quot;, k, sep = &quot;&quot;), 
       xlab = &quot;Time&quot;, bty = &quot;n&quot;)
}</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-24-1.png" width="90%" /></p>
<p>Plot the first 50 eigenvalues from the largest to the smallest</p>
<pre class="r"><code>par(mar = c(4.5, 4, 1, 1))
plot(raw_eigenvalues[1:50], bty = &quot;n&quot;, type = &quot;l&quot;, lwd = 3, ylab = &quot;Eigenvalues&quot;, xlab = &quot;Eigenvalues Index&quot;)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-25-1.png" width="90%" /></p>
</div>
<div id="pca-for-randomly-placed-bumps" class="section level3">
<h3>PCA for randomly placed bumps</h3>
<p>We generate data independently data without noise where</p>
<p><span class="math display">\[W_i(t) =
\frac{1}{\sqrt{2\pi}\sigma_i}\exp\left\{-\frac{(t-\mu_i)^2}{2\sigma^2_i}\right\}\;,\]</span>
where <span
class="math inline">\(\mu_i\sim\textrm{Uniform}[0,1]\)</span> are random
centers of the Gaussian distributions. The standard deviations are <span
class="math inline">\(\sigma_i\sim\textrm{Uniform}[0.05,0.15]\)</span>,
which control the size of the bump.</p>
<pre class="r"><code>W &lt;- matrix(rep(NA, p * n), ncol = p)
x &lt;- seq(0, 1, length = 3000)

for(i in 1:n){ 
  mu &lt;- runif(1)
  sigma &lt;- 0.01
  W[i,] &lt;- dnorm(x, mu, sigma)
}</code></pre>
<p>Plot the first 5 functions generated via this procedure</p>
<pre class="r"><code>library(viridis)
col.sc &lt;- viridis(10)
plot(1, type = &quot;n&quot;, xlab = &quot;Functional domain&quot;, ylab = &quot;Functional values&quot;, 
     xlim = c(0, 1), ylim = c(0, 40), bty = &quot;n&quot;)
for(i in 1:10){
  lines(x, W[i,], type = &quot;l&quot;, lwd = 2, col = col.sc[i])
}
lines(c(0, 1), c(0, 0), lwd = 3)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-27-1.png" width="90%" /></p>
<pre class="r"><code>results_raw &lt;- prcomp(W)

raw_eigenvalues &lt;- results_raw$sdev ^ 2 / 3000

#Obtain the estimated eigenvectors
#Normalize to match with the eigenfunctions
raw_eigenvectors &lt;- sqrt(p) * results_raw$rotation

par(mar = c(4.5, 4.5, 2, 2))
par(mfrow = c(K / 2, 2))
seq &lt;- (1:(p / 10)) * 10
for(k in 1:K){
  plot(t, raw_eigenvectors[,k], type = &quot;l&quot;, lwd = 2, ylim = c(-3, 6), 
       ylab = paste(&quot;Eigenfunction &quot;, k, sep = &quot;&quot;),
       xlab = &quot;Time&quot;, bty = &quot;n&quot;)
}</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-28-1.png" width="90%" /></p>
<p>Plot the first <span class="math inline">\(50\)</span> largest
eigenvalues for the Gaussian bumps data.</p>
<pre class="r"><code>par(mar = c(4.5, 4, 1, 1))
plot(raw_eigenvalues[1:50], bty = &quot;n&quot;, type = &quot;l&quot;, lwd = 3, ylab = &quot;Eigenvalues&quot;, xlab = &quot;Eigenvalues Index&quot;)</code></pre>
<p><img src="chapter_03_files/figure-html/unnamed-chunk-29-1.png" width="90%" /></p>
<p>The number of study participants is set to <span
class="math inline">\(n=250\)</span> and the number grid points is set
to <span class="math inline">\(p=3000\)</span> to illustrate a case of
high dimensional data.</p>
</div>
</div>

<br><br>
<footer>
  <p class="copyright text-muted" align="center">Copyright &copy; 2023</p>
</footer>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
